{
    "article_id": "4970fd21-7daa-46c7-98ea-2f07a08d91b5",
    "extracted_text": "www.nature.com/npjdigitalmed\nARTICLE OPEN\nA simulation-based evaluation of machine learning models for\nclinical decision support: application and analysis using\nhospital readmission\n\u2709\nVelibor V. Mi\u0161i\u0107 1 , Kumar Rajaram1 and Eilon Gabel 2\n;,:)(0987654321\nINTRODUCTION algorithm\u2019spredictions11.Thepresenceoftheproviderintroduces\nTheworld ofclinical medicinehas shown tremendous interest in several important operational considerations that potentially\nartificial intelligence and machine learning1. Whereas machine complicate how the value of a machine learning algorithm is\nlearning algorithms are now commonplace in most other assessed, in terms of patient outcomes and the cost savings\nindustries, these technologies have more recently been studied associated with mitigating the adverse event. These considera-\nin medicine and have ignited a passion to find non-traditional tions include constrained access to providers, prediction timing\nwaystoimprovepatientoutcomes2.Althoughmanyofthesame relative to pathway resource availability, provider costs, the\nmachine learning methodologies used in other domains are also success rate of providers\u2019 interventions, and the potential cost\nused in healthcare, there are important differences in implemen- savingsofthe pathwayinterventions.\ntation3. In domains such as e-commerce, a machine learning Typically, healthcare machine learning models are evaluated\nalgorithm is often used to make low-stakes decisions without using traditional machine learning metrics. For example,\nhuman supervision4. For example, Google\u2019s Adwords platform receiver\u2013operator characteristic curves and the area under the\nreceiver\u2013operatorcharacteristiccurve(AUROC;alsoknownasthe\nusesmachinelearningtopredicttheprobabilityofauserclicking\non an ad, which is then used to decide which ads to display in c statistic) are considered gold standards for quantifying\nresponse to a user\u2019s query5. Due to the high volume of queries predictive performance12\u201314. Other widely-used metrics include\nandthefactthatwrongdecisionsdonotaffectaperson\u2019slife,the calibration-related metrics (such as the calibration slope and\nAdwords platform directly makes such decisions, without human intercept)15, the net reclassification improvement16, and inte-\noversight. grated discrimination improvement16. While these metrics and\nIn contrast, machine learning algorithms employed in health- methods are valuable for understanding predictive performance,\ncaretypicallyinvolvepredictingwhetherapatientwillexperience they lack clinical interpretability because they do not consider\nsome type of adverse event6\u20138. Machine learning algorithms in resource constraints, and do not consider the impact of the\nhealthcareareusedtoaffectanindividual\u2019sdiseasecourse,which prediction made by the algorithm on the patient\u2019s health.\ndiffers from how machine learning algorithms are used in other Specifically, they do not incorporate the providers\u2019 schedules\napplications(suchasonlineadvertising)totargetpopulations.The and the constraints on how many patients can be physically\ndecisions that machine learning algorithms in healthcare inform enrolledinapathway;thetimingofwhenthepredictionbecomes\nare clinically important: an incorrect prediction of a patient\u2019s risk available to the providers; the cost of the providers; and the\ncan result in the patient receiving the wrong type of care or not benefit of the providers both in terms of how many adverse\nreceiving any care at all9. An algorithm\u2019s predictions are thus events the providers can prevent, and the cost savings of this\ntypically reviewed by a clinician/provider and used to support prevention. Although having merit, these commonly used\neffective decision making10. An effective algorithm or model will performance metrics cannot be solely used to completely\ncorrectlyanticipatepatientswhowillexperienceanadverseevent understandtheoverallbenefit,bothintermsofmanagingpatient\nand allow time for a provider to potentially prevent the adverse outcomes and managing costs, of a given machine learning\neventfromoccurring inthefirst place. model.Besidesthesepredictivelyfocusedmetrics,anotablebody\nThus, a critical factor in the deployment of any machine ofrecentworkhasapproachedpredictivemodelsfromadecision-\nlearningmodelinhealthcareistheprovider,whoisguidedbythe analytic perspective, and has proposed the metric of net benefit\n1Decisions, Operations and Technology Management, Anderson School of Management, University of California Los Angeles, Los Angeles, CA, USA. 2Department of\n\u2709\nAnesthesiologyandPerioperativeMedicine,UniversityofCaliforniaLosAngeles,LosAngeles,CA,USA. email:velibor.misic@anderson.ucla.edu\nPublishedinpartnershipwithSeoulNationalUniversityBundangHospitalVelibor.V.Mi\u0161i\u0107etal.\n2\nand the associated concept of a decision curve17. While these\napproaches help to compare different models, these approaches\nagain do not account for resource constraints. The development\nof clinically applicable metrics and the integration of machine\nlearning into clinical workflows have been identified as major\nchallengesintheimplementationofhealthcaremachinelearning\nmethods18\u201321, alongside other issues such as logistical difficulties\ninaccessingdatafromarangeofsources18,biasintrainingdata19,\nwhethermodeltrainingshouldbesite-specific22,andalgorithmic\ninterpretability23. In addition, recently proposed frameworks for\ntranslating machine learning into clinical practice have high-\nlighted the demonstration of economic utility20 and rigorous\nevaluation of impact in terms of clinical outcomes and cost21 as\nimportant steps; such rigorous evaluation is likely to be of\nincreasing importance in the future as machine learning-based\nsoftware-as-a-medical-device (SaMD) systems become more\nregulatedand standardized24.\nIn this paper, we propose a simulation model that allows\ncliniciansandhospitalleadershiptoassessthevalueofamachine\nlearning model in a dynamic, provider-constrained setting. We\nspecifically study the problem of using machine learning to\npredict postoperative 30-day hospital readmission via an emer-\ngencydepartment,andcomparepreviouslydevelopedpredictive\nmodels that differ in predictive performance (as measured by\nAUROC) and in prediction timing (whether the prediction is\n;,:)(0987654321\navailable as early as the completion of surgery, or whether it is\nonly available on the day of discharge). Our simulation model\ninvolves tracking the flow of patients as they complete surgery\ncharacteristics of this patient data set. Based on the extracted\nthrough discharge; calculating the risk of each patient using the\ndata,thesimulationhorizonwasdefinedasaperiodof847days;\npredictivemodel; andthenselectingthehighest riskpatientsfor\nintervention,subjecttotheprovider\u2019savailabilityandthepathway wenotethatthisperiodisslightlylongerthantwoyears,assome\npatients stay in the hospital past the end of 2018. For more\ncapacity (i.e., the maximum number of patients the provider can\ninformation, we refer readers to the Methods section, which\nengage with on a given day). In this way, our simulation model\ndescribes our data extraction process, our simulation, and our\nalignsthepredictionsproducedbyamachinelearningmodelwith\nsimulation-based performance metrics ingreater detail.\ntheprovider.\nTo identify the patients satisfying our definition of a 30-day\nAt a high level, our simulation model is intended for clinicians\nemergency department readmission, we first identified 1784\nand hospital leadership who are interested in implementing a\nadmissions that were followed by a subsequent 30-day emer-\nmachine learning model in real-time to guide the allocation of a\ngency department visit. Of these, 36 were excluded due to the\nconstrainedresource.Ideally,thissimulationwouldbeusedearly\nemergencydepartmentvisit occurringon thesame calendarday\nin the model development process, when one has already\nasdischarge,andafurther779wereexcludedfornotresultingin\ndeveloped a candidate set of models, and is interested in\na transfer to a subsequent non-emergency department location.\nforecasting the cost and prevention outcomes of each model in\nThisresultedinasetof969admissions(5.0%ofthecompleteset\nordertodecideonthemostsuitableone.Whilewedemonstrate\nof 19331 admissions) with readmission via the emergency\noursimulationmodelinthecontextofpostoperativepredictionof\ndepartment.\nreadmissionforsurgicalpatients,itappliestoanyotherMLmodel\nthatis used toallocate aconstrained resource.\nOurresultshighlightthecomplexityofusingmachinelearning Existingpredictiveanddecision-analyticperformancemetrics\nin clinical scenarios. In the context of readmissions, while better Table1displaysthepredictiveanddecision-analyticperformance\npredictiveperformancetranslatestobetterimpactintermsofthe metrics forthefour predictive models.Thistablehasfour sets of\nnumber of readmissions anticipated, this impact also highly rows,foreachofthefourpredictivemodels.Intermsofmetrics,it\ndepends on provider pathway enrollment and prediction timing displaystheAUROC;thediscretenetreclassificationimprovement\nto increase the patients\u2019 window of availability. In addition, we (NRI),measuredrelativetotheHOSPITALscoreanddefinedusing\nshow that the net cost savings also depend on these factors, as theriskintervals[0,0.1),[0.1,0.2),[0.2,0.3),[0.3,1];thecontinuous\nwell as the provider cost and the effectiveness of the implemen- NRI, measured relative to the HOSPITAL score; the integrated\ntedpathway.Ultimately,oursimulationmodelprovidesclinicians discrimination improvement (IDI), measured relative to the\nand administrators with a more clinically relevant assessment of HOSPITAL score; and the net benefit (NB), measured using a\nthe value of any predictive model in terms of patient outcomes probabilitycutoffof0.1.FromtheAUROCmetric,wecanseethat\nandcost. HOSPITALattainsthelowestAUROC(0.7169),followedbythenon-\nlab-based L1 logistic regression model, followed by LACE, and\nfinally,thelab-basedL1logisticregressionmodel,whichattainsan\nRESULTS\nAUROC of 0.8541. The two NRI metrics and the IDImetric yielda\nDataextraction similar ordering of the models: the two L1 logistic regression\nRecordsfor19343surgicaladmissionstotheUCLARonaldReagan modelsandtheLACEmodelallachieveanimprovementoverthe\nMedicalCenter(RRMC)overtheperiod2017\u20132018wereextracted. HOSPITAL model in reclassification and discrimination, with the\nOf these, 12 admissions were excluded for being terminal organ lab-based L1 logistic regression model achieving the highest\ndonors. The remaining 19331 admissions were used as the input improvement,andthenon-lab-basedL1logisticregressionmodel\ndata to our simulation. Supplementary Table 1 summarizes the achieving the smallest improvement. With regard to the NB\nnpjDigitalMedicine(2021) 98 PublishedinpartnershipwithSeoulNationalUniversityBundangHospitalVelibor.V.Mi\u0161i\u0107etal.\n3\nTable2. Simulation-basedpatientperformancemetricsforthefourmodels.\nSchedule Capacity Method PS RA ERP\nM 8 L1LR(withlabs) 866 258 25.8\n(858,874) (228,281) (22.8,28.1)\nM 8 L1LR(nolabs) 861 155 15.5\n(855,869) (133,176) (13.3,17.6)\nM 8 LACE 845 91 9.1\n(832,849) (67,101) (6.7,10.1)\nM 8 HOSPITAL 845 86 8.6\n(832,849) (67,101) (6.7,10.1)\nMW 8 L1LR(withlabs) 1705 423 42.3\n(1696,1713) (381,454) (38.1,45.4)\nMW 8 L1LR(nolabs) 1699 263 26.3\n(1691,1707) (229,289) (22.9,28.9)\nMW 8 LACE 1688 178 17.8\n(1673,1695) (150,198) (15,19.8)\nMW 8 HOSPITAL 1688 173 17.3\n(1673,1695) (142,190) (14.2,19.0)\nMTWRF 8 L1LR(withlabs) 4199 672 67.2\n(4189,4213) (629,720) (62.9,72)\nMTWRF 8 L1LR(nolabs) 4196 502 50.2\n(4186,4207) (461,548) (46.1,54.8)\nMTWRF 8 LACE 4215 456 45.6\n(4190,4222) (405,486) (40.5,48.6)\nMTWRF 8 HOSPITAL 4215 437 43.7\n(4190,4222) (375,450) (37.5,45.0)\nUnder\u201cMethod\u201d,\u201cL1LR\u201ddenotesL1regularizedlogisticregression.\u201cSchedule\u201dindicatesthedaysoftheweekonwhichtheproviderworks(M=Monday,T=\nTuesday,W=Wednesday,R=Thursday,F=Friday).Forthelastfourcolumns,PSdenotesthenumberofpatientsseen(howmanypatientswereseenbythe\nprovideroverthesimulationhorizon);RAdenotesthenumberofreadmissionsanticipated(howmanypatientswereseenbytheproviderandhadanER\nreadmission);andERPdenotestheexpectedreadmissionsprevented(RAmultipliedbytheeffectivenesscoefficient),assumedtobe10%.The95%bootstrap\nconfidenceinterval(basedon1000bootstrapsimulations)isshownunderneatheachmetric.\nmetric, we can see that at a probability cutoff of 0.10, the applied on the discharge date. For example, if the provider only\nHOSPITAL,LACE,andnon-lab-basedL1logisticregressionmodels sees patients on Mondays and a patient is discharged on a\nachieve similar net benefits of 0.49%, 0.50%, and 0.53%, Tuesday, that patient will not be selected by a provider using\nrespectively, and the lab-based L1 logistic regression model HOSPITAL orLACE.\nachieves the highest net benefit of 1.8%. This ordering of the The readmissions anticipated (RA) metric measures how many\nmodelsisagainqualitativelysimilartothatobtainedfromAUROC. of those patients seen by the provider have unplanned read-\nmission. When the provider sees patients on Mondays and uses\nSimulation-based patientperformance metrics theL1regularizedlogisticregressionmodelwithoutthelab-based\nfeatures, the provider correctly anticipates 155 readmissions.\nTable2displaysthesimulation-basedpatientperformancemetrics\nWhentheproviderusestheL1logisticregressionmodelwiththe\nforthefourpredictivemodelsoverthesimulationhorizon(seethe\n\u201cPerformance metrics\u201d section under the \u201cMethods\u201d section for a lab-based features and follows the same schedule, the provider\nprecise definition of each of the metrics). This table shows three correctly anticipates 258 readmissions; relative to the non-lab-\ndifferent provider schedules: in the first, the provider sees 8 based model, this constitutes an improvement of 66%. Using\npatients once a week every Monday; in the second, the provider HOSPITAL and LACE only results in 86 and 91 readmissions,\nsees8patientsonMondayand8onWednesday;andinthethird, respectively, being correctly anticipated with the same Monday-\nthe provider sees 8 patients every day from Monday to Friday. only schedule; relative to HOSPITAL and LACE, the lab-based L1\nThe patients seen (PS) metric measures how many patients the logisticregressionmodelresultsinanimprovementofover183%.\nproviderseesoverthesimulationhorizon.Forexample,whenthe The difference between the lab-based and the non-lab-based\nprovider sees 8 patients every Monday, the four different model is driven by the difference in their predictive ability. As\npredictive models result in the provider seeing between 845 showninTable1,theAUROCforthelab-basedmodelonthisdata\npatients (for HOSPITAL and LACE) and 866 patients (for L1 set is 0.8541, whereas it is only 0.7280 for the non-lab-based\nregularizedlogisticregressionwiththelab-basedfeatures).Asthe model.AmodelwithaperfectAUROCof1wouldensurethatthe\nprovider\u2019s schedule includes more days, the PS metric increases. numberofreadmissionsanticipatedbytheproviderismaximized,\nEvaluating the Monday\u2013Wednesday schedule, the number of subjectto theconstraint imposed by theprovider\u2019s schedule.\npatients seen ranges from 1688 to 1705, whereas with the The difference between the non-lab-based model versus\nMonday-to-Friday schedule 4196\u20134215 patients are seen. In HOSPITAL and LACE, however, is not due to predictive ability,\ngeneral, HOSPITAL and LACE result in a smaller number of because the non-lab-based model, HOSPITAL, and LACE all\npatients seen, because these two predictive models can only be achieve AUROCs in the range 0.71\u20130.74. The difference arises\nPublishedinpartnershipwithSeoulNationalUniversityBundangHospital npjDigitalMedicine(2021) 98Velibor.V.Mi\u0161i\u0107etal.\n4\nTable3. Simulation-basedcostperformancemetricsforthefourdifferentmodels.\nSchedule Capacity Method ERC PC ERCS ENCS\nM 8 L1LR(withlabs) 3,745,900 72,600 374,590 301,990\n(3310498,4078118) (331050,407812) (258450,335212)\nM 8 L1LR(nolabs) 2,232,000 72,600 223,200 150,600\n(1914840,2534760) (191484,253476) (118884,180876)\nM 8 LACE 1,320,900 72,600 132,090 59,490\n(1015000,1540878) (101500,154088) (28900,81488)\nM 8 HOSPITAL 1,252,400 72,600 125,240 52,640\n(975290,1471510) (97529,147151) (24929,74551)\nMW 8 L1LR(withlabs) 6,142,700 145,200 614,270 469,070\n(5528685,6579343) (552869,657934) (407669,512734)\nMW 8 L1LR(nolabs) 3,790,700 145,200 379,070 233,870\n(3297600,4167420) (329760,416742) (184560,271542)\nMW 8 LACE 2,577,200 145,200 257,720 112,520\n(2170413,2865298) (217041,286530) (71841,141330)\nMW 8 HOSPITAL 2,513,300 145,200 251,330 106,130\n(2058753,2762815) (205875,276282) (60675,131082)\nMTWRF 8 L1LR(withlabs) 9,776,000 453,750 977,600 523,850\n(9153885,10474863) (915389,1047486) (461639,593736)\nMTWRF 8 L1LR(nolabs) 7,265,500 453,750 726,550 272,800\n(6666153,7921670) (666615,792167) (212865,338417)\nMTWRF 8 LACE 6,618,400 453,750 661,840 208,090\n(5861268,7052718) (586127,705272) (132377,251522)\nMTWRF 8 HOSPITAL 6,350,800 453,750 635,080 181,330\n(5441198,6520680) (544120,652068) (90370,198318)\nUnder\u201cMethod\u201d,\u201cL1LR\u201ddenotesL1regularizedlogisticregression.\u201cSchedule\u201dindicatesthedaysoftheweekonwhichtheproviderworks(M=Monday,T=\nTuesday,W=Wednesday,R=Thursday,F=Friday).Forthelastfourcolumns,ERCdenotestheexpectedreadmissioncostindollars(thesumoftheexpected\nreadmissioncost,accordingtoHCUPdata,forthosepatientsseenandwhohadanERreadmission);PCdenotestheprovidercostindollars(thecostofgiving\ntheprovider).\nbecause of the schedule and the availability window. With enroll all of the surgical patients without provider cost concerns,\nHOSPITAL and LACE, the provider is restricted to only seeing thenallfourpredictivemodelswouldbenefitpatientsidentically.\npatientsthatarebeingdischargedonapathwayenrollmentday. The improvements in the readmissions anticipated by the L1\nThus,whiletheremaybehigh-riskpostoperativepatientsthatare logisticregressionmodelsdirectlytranslateintoimprovementsin\navailable and that could in theory be seen, the provider must the ERP metric, which measures the expected number of\nselectfromamongthosethatarebeingdischarged,andthesetof readmissions prevented. For example, assuming an effectiveness\npatients selected by the provider will include patients that are constant of 10% and the provider seeing up to 8 patients every\nlow-risk. In contrast, with the non-lab-based model, the provider Monday,thelab-basedL1logisticregressionmodelisexpectedto\ncan see a patient on any day from the completion of surgery to prevent 26 readmissions, whereas HOSPITAL and LACE are\ndischarge. Thus, on any given Monday, the provider can select expected toprevent only9.\npatientswhorecentlycompletedsurgeryandarehigh-risk,aswell\nas high-risk patients who have already been in the hospital for\nSimulation-based costperformance metrics\nsometime and have notyet beenseen.\nTable3reportsonthecostperformancemetricsforthedifferent\nAs discussed above, under a Monday-only schedule, the lab-\npredictivemodelsunderthesamethreeproviderschedulesasin\nbased model improves on the readmissions anticipated over the\nTable 2. (The precise definition of each metric is provided under\nnon-lab-based model, HOSPITAL and LACE. When the provider\nsees patients on Mondays and Wednesdays, this benefit the\u201cPerformance metrics\u201dsubsection ofthe\u201cMethods\u201dsection.)\ndecreases: the lab-based model improves on the readmissions For the Monday-only schedule, the lab-based L1 logistic\nanticipatedbyabout60%overthenon-lab-basedmodel(423vs. regression model selects a set of patients over the simulation\n263), and by about 138% over HOSPITAL and LACE (423 vs. 173 horizon whose expected readmission cost is approximately $3.7\nand 178). When the provider sees patients on Monday through million. This is larger than the readmission cost of the patients\nFriday,thisbenefitfurtherdecreasestoabout34%relativetothe selected by the non-lab-based L1 logistic regression model ($2.2\nnon-lab-based model (672 vs. 502), about 54% relative to million), as well as HOSPITAL and LACE ($1.3 and $1.3 million,\nHOSPITAL (672 vs. 437) and about 47% relative to LACE (672 vs. respectively).AsinTable2,thedifferencebetweenthelab-based\n456). The reason that this benefit decreases is because with a and non-lab-based L1 logistic regression models is driven by the\nlargerschedule,theprovidercanenrollmorepatientsandisless differenceintheirpredictiveability,whilethedifferencebetween\nconstrained; stated differently, any deficiencies in a model\u2019s the non-lab-based L1 logistic regression model and the two\npredictive ability can be compensated for by seeing more scoring rules (HOSPITAL and LACE) is driven by the difference in\npatients. Taken to the extreme, if providers could theoretically the availability window. The provider cost for this schedule is\nnpjDigitalMedicine(2021) 98 PublishedinpartnershipwithSeoulNationalUniversityBundangHospitalVelibor.V.Mi\u0161i\u0107etal.\n5\nWhen we consider the other schedules\u2014Monday/Wednesday\nandMonday-to-Friday\u2014weobservethatasthescheduleenlarges,\ntheexpectedreadmissioncostandtheexpectedreadmissioncost\nsavings increase, because the provider sees more patients and\nanticipatesmorereadmissions.Theprovidercostalsoincreases,as\nthe provider works a larger number of hours. The relative\ndifference in expected net cost savings between the lab-based\nL1logisticregressionmodelandHOSPITAL/LACEdecreasesasthe\nprovider\u2019s schedule enlarges. This mirrors our observations with\ntheRAmetricinTable2,wherewesawthatseeingmorepatients\nleadstoa reducedperformance gap between themethods.\nFigure1plotstheexpectednetcostsavingsasafunctionofthe\neffectiveness constant for the four predictive models, for each of\nthe three schedules. These plots suggest that ifthe effectiveness\nconstantissufficientlylow,thenitisnotcost-effectivetousethe\nprovider. The plots also allow us to infer the break-even\neffectiveness constant for each of the models. For example, for\nthe Monday-only schedule, the lab-based L1 logistic regression\nmodel leads to positive net cost savings when the effectiveness\nconstant is roughly 2% or higher; in contrast, for HOSPITAL and\nLACE,oneneedstheeffectivenessconstanttobe>5.5\u20136%forthe\nnetcostsavingstobecomepositive.Theseplotsalsosuggestthat\nthe break-even effectiveness constant generally increases as the\nproviderscheduleisenlarged.Thisimpliesthatforpathwayswith\na low effectiveness constant, one should consider a smaller\nproviderscheduletostayprofitable;conversely,alargerprovider\nschedule is only justified when the pathway has a higher\neffectivenessconstant.\nAdditional results\nIn the Supplementary Information, we provide patient and cost\nperformance results for alternate values of 5 and 20% for the\neffectiveness constant, with the same provider cost model\n(Supplementary Tables 2 and 3). In Supplementary Table 4, we\nalso report cost results under two different provider cost\nscenarios, where we change the provider cost by factors of 0.5\nand 2.0 (provider cost is half and twice the base case in Table 3,\nrespectively). Lastly, in Supplementary Table 5, we consider an\nalternateprovidercoststructure,wheretheprovidercostisbased\nona cost perpatient selected(oneof $100, $200,or $300).\nFig.1 Expectednetcostsavingsasafunctionoftheeffectiveness\nconstant,forthethreedifferentschedules.Under\u201cMethod\u201d,\u201cL1LR\u201d DISCUSSION\ndenotesL1regularizedlogisticregression.\u201cSchedule\u201dindicatesthe Our simulation model yields three important insights. The first is\ndays of the week on which the provider works (M = Monday,\nthatimprovementsinthepredictiveperformanceofareadmission\nT = Tuesday, W = Wednesday, R = Thursday, F = Friday). Panel a\npredictionmodeldirectlytranslatetomoreeffectiveallocationsof\ncorresponds to a Monday-onlyschedule,panel b corresponds to a\nlimitedreadmissionpreventionresources.Thisisborneoutinthe\nMonday/Wednesday schedule, and panel c corresponds to a\nMonday-Fridayschedule. comparison between the lab-based and non-lab-based L1\nregularized logistic regression model, which have the same\navailability window and attain AUROCs of 0.8541 and 0.7280,\nrespectively.Bothmodelsresultintheproviderseeingroughlythe\n$72,600,whichwenoteisthesameacrossallfourmodels.Under\nsame number of patients in each schedule, but the lab-based\ntheassumptionofaneffectivenessconstantof10%,theexpected\nmodel anticipates a much larger number of readmissions, and\nreadmissioncostsavingsforthefourmodelsrangesfromroughly results in significantly higher expected net cost savings. Our\n$380,000forthelab-basedL1logisticregressionmodeltoroughly\nsimulation model allows clinicians and hospital leadership to\n$125,000and$132,000forHOSPITALandLACE,respectively.The directly see how differences in AUROC will translate into the\nexpectednetcostsavings,whichisthedifferenceoftheexpected added readmissions that a prevention pathway correctly antici-\ncostsavingsandtheprovidercost,rangesfromroughly$302,000 pates, and the increased cost savings from correctly anticipating\nforthelab-basedL1logisticregressionmodel,toroughly$50,000\u2013\nthoseadditional readmissions.\n$60,000forHOSPITALandLACE.Weemphasizethattheseresults The second insight is that the timing of a prediction, as\narebasedontheassumptionofaneffectivenessconstantof10%; captured through the patients\u2019 availability window, also impacts\nunder different values of the effectiveness constant, these the ultimate efficacy of how providers can enroll patients into a\nexpected net cost savings can change drastically (as we will preventionpathway.Inourresults,thenon-lab-basedregularized\nsubsequently show; cf. Fig. 1). Notwithstanding this assumption, logistic regression model, HOSPITAL and LACE all achieve\nthese results illustrate how differences in both predictive ability comparable AUROCs, but HOSPITAL and LACE anticipate a much\nandtheavailabilitywindowofamodelcandirectlytranslateinto smaller number of readmissions, because those two models can\ndifferences inexpected cost and financialperformance. only be applied to a patient on the day of discharge, while the\nPublishedinpartnershipwithSeoulNationalUniversityBundangHospital npjDigitalMedicine(2021) 98Velibor.V.Mi\u0161i\u0107etal.\n6\nnon-lab-based regularized logistic regression model can be our Supplementary Information for alternate values of 5 and 20%).\nappliedassoonasthepatientcompletesthesurgery.Thisdirectly We note that the recent paper of Leppin et al.25 presents a meta-\ntranslates tolower expected netcost savings.Whileitis intuitive analysisofstudiesonreadmissionpreventionandsuggeststhatthe\nthat having access to a prediction earlier is better than later, the average reduction in readmissions is 18%. Although this number is\nvalueofoursimulationmodelisinquantifyingtheimpactofthis anaverage over studies that have considered different populations\nvariable in terms of patient care (how many readmissions are and different types of interventions, this would suggest that our\nanticipated) and cost (expectednetcost savings). results, which are based on a lower effectiveness constant value of\nThe third insight is that resource constraints and costs also 10%, are a conservative estimate of the efficacy of the different\nimpactmodelefficacy.Ourresultsshowthattheimprovementin models. It is straightforward to adjust the simulation model details\nthe number of readmissions anticipated by the two machine (suchastheprovidercapacityandtheinterventioneffectiveness)to\nlearningmodelsoverHOSPITALandLACEishighestforthemost more accurately characterize a machine learning model\u2019s perfor-\nconstrainedschedule,anddecreasesastheprovider\u2019sscheduleis mance.Inaddition,itisalsostraightforwardtomodifythesimulation\nexpanded to enroll a larger number of patients. Thus, in model to incorporate a patient-specific effectiveness constant that\nquantifying the value of improved predictive performance, our would depend on the characteristics of the patient and their index\nresults suggest that one must account for the capacity of admission.\nresources that will be directed by a predictive model. Through Second, our analysis makes assumptions on the costs of\nour simulation model, a clinician that is deciding how to best readmissions and the cost of setting up the provider pathway.\ndeploy a given machine learning model can evaluate multiple The readmission costs are derived from Healthcare Cost and\nschedules and choose the schedule that leads to the most UtilizationProject(HCUP)data,whichrepresentnationalaverages\nattractive performance in terms of provider cost, readmissions anddonotsolelyfocusonsurgicalpatients;theactualcostsata\nanticipated, and expected netcost savings. given institution could differ significantly. Given more detailed\nTaken together, these three insights suggest that while andhigherfidelitydataonthecostofeachreadmission,itisagain\nstandard metrics used in healthcare machine learning (such as straightforward to modify the simulation model to derive more\nAUROC) are helpful for characterizing predictive performance, precise estimates of net cost savings. With regard to the cost of\nthese metrics do not fully quantify the value of models in thepreventionpathway,wenotethatitisalsoeasytomodifythe\npotentially improving patient outcomes and reducing costs. In simulationmodeltoaccommodatemorecomplexcoststructures.\nparticular, a model\u2019s AUROCvaluedoes not capture any element For example, in the Supplementary Information, we consider an\nof when the model\u2019s predictions become available; it does not alternatecoststructurewherethecostisavariablecostthatscales\naccountforthescheduleoftheproviderthatwillusethemodelto linearly in the number of patients enrolled in the intervention\ndirect, and it does not account for the cost associated with the pathway.Ouranalysisalsodoesnotincludethecostofacquiring\nprovider and the expected savings associated with each read- thedataandsettingupthealgorithmforreal-timeuse.Although\nmission. As a stark example of the importance of costs and developingthedatawarehousingcapabilitytosupportareal-time\nsavings, our sensitivity analyses suggest that if the effectiveness algorithm would be an additional upfront cost, such an\ndecreasedto5%(asinSupplementaryTable3)oriftheprovider investmentcouldpotentiallybeamortizedoverthedevelopment\ncostweretwiceashigh(SupplementaryTable4),theexpectednet ofmany other clinicaldecisionsupport tools.\ncost savings generated by HOSPITAL and LACE in all three Third, our simulation model does not address logistical issues in\nschedules are negative; under such scenarios, our simulation theimplementationofthemachinelearningmodel,andinparticular,\nmodel would suggest that there is no financial justification to itdoesnotconsiderhowthemachinelearningmodelwillfitwiththe\nusingHOSPITAL and LACEtoguideprovider allocation.Thistype existingclinicalworkflow.Asnotedpreviously,theapplicationofour\nofinsightisabsentfromsolelypredictivemetricslikeAUROC,but simulationmodeltothecontextofreadmissionpreventionassumes\nisof critical importance forimplementation. that there is a dedicated provider guided by a machine learning\nUltimately,predictivemodelsexisttoenabledecisionmakersto model who is able to see a certain number of patients on certain\njudiciously allocate limited resources over time so as to enroll days of the week. This workflow does not currently exist at UCLA\npatients in care pathways, improve quality of care, and reduce RRMC,andwearenotawareofsuchasimilarworkflowelsewhere.\ncosts. Although we have focused on readmission prediction, Thus, it is difficult to foresee potential issues with relaying a\nresource constraints and limited intervention time windows are predictive model\u2019s predictions to a provider at this point in time.\nessential features of many other risk prediction problems that Studying how the predictions from the machine learning model\nariseinhealthcarewhenrollingoutclinicalpathways.Ourresults shouldberelayedtotheprovider(e.g.,whentheprovidershouldbe\nhighlight the importance of simulating any predictive model given those predictions, and how much auxiliary information the\nunder consideration in a simulation model that accounts for the provider should be given), is an important direction for future\ntemporaldynamicsofpatientflow,aswellastheconstraintsthat research.Oursimulationmodelalsodoesnotaccountforthenature\nlimit when and to what extent resources can be used to affect ofthemachinelearningmodelintermsofissueslikeinterpretability,\npatients.Oursimulationmodelcanthusbeviewedasastepping trust, bias, and confounding. While these issues are typically\nstone from initial machine learning model development\u2014which addressed in the model development stage, they are important to\ninvolves defining features, model estimation, and out-of-sample considerbecausetheycanaffecttheadherenceofproviderstothe\ntesting\u2014tothereal-timeimplementationofthepredictivemodel machine learning model\u2019s recommendations, and the potential\nwithinanelectronichealthrecord(EHR)system.Moreimportantly, information that the provider can glean from the model. For\nitcanbeusedasaguideforprovidingfinancialjustificationwhen example,ifthepredictionsareproducedbyaninterpretablemodel,\nproposingAI-backed clinicalpathways tohospital leadership. theprovidercanseehowthepredictionismadeforagivenpatient\nThereareseverallimitationstoouranalysis.First,weassumethatif andcanpotentiallyobtaininsightintotheunderlyingcausesofthe\naproviderseesapatientthatwouldeventuallygetreadmitted,that patient\u2019sincreasedriskofreadmission,whichcanhelptheprovider\ntheprovidercanpreventthereadmissionofthatpatientwithsome withapplyingtheintervention.\nprobability,whichisgivenbytheeffectivenessconstant.Whilethere Lastly, we note that our simulation model compares different\nismuchdataontechniquestopreventhospitalreadmission,weare predictive models under different provider schedules in terms of\nintheprocessofcreatingasoundpathwaywithnothingyettrialed the number of readmissions prevented and in terms of the net\nonpatients.Wedonotmakeassumptionsonthetypeofpathway; costsavingsofthosereadmissionsprevented.Anotherimportant\nwe only assume that the pathway\u2019s effectiveness constant is dimensiontoconsideristheeffectoftheprovideronthepatients\u2019\nuniformly 10% across all patients (and provide additional results in health, in terms of clinical outcomes and the patient\u2019s quality of\nnpjDigitalMedicine(2021) 98 PublishedinpartnershipwithSeoulNationalUniversityBundangHospitalVelibor.V.Mi\u0161i\u0107etal.\n7\nlife.Thisraisesaconcernthattheprovidercouldmakechangesin regressionmodelcanchangewitheachdayintheavailabilitywindow.We\na patient\u2019s care that would negatively impact the outcome and updatedthepredictionafterday1(thedayofsurgerycompletion)andday\ncause a readmission that was not likely prior to the patient 2(onedayaftersurgerycompletion)oftheavailabilitywindow.Wedidnot\nengagement.Thisdimensionofconsideringthepatient\u2019shealthis update the prediction after day 2 of the availability window, as it was\nshowninMi\u0161i\u0107etal.28thattheAUROCperformanceofthemodelplateaus\ncurrently not captured in our simulation model; including it\nat1.5\u20132daysafterthecompletionofsurgery.\nconstitutesan important direction offuture research.\nAn essential variable required by both HOSPITAL and LACE is the\nIn conclusion, we proposed a new simulation model to patient\u2019s length of stay, which will typically not be known upon the\ndemonstratetherichclinicalandadministrativevalueofmachine completionofsurgery.Thus,weassumedthattheavailabilitywindowfor\nlearningmodelsthatisnotcapturedbygoldstandardmetricsfor HOSPITALandLACEwouldconsistofonlyasingleday,whichisthedayof\npredictiveperformance,suchasAUROC.Asthisworkevolves,we discharge.\nhope to transform this into a true patient trial to compare our\nsimulated in-silicoresults against real in-vivooutcomes. Simulated provider. We assumed a single provider following a weekly\nschedulethatdefinesasetofdaysonwhichtheproviderisabletosee\npatients.Oneachdayintheschedule,theproviderwasassumedtohavea\nMETHODS limitonthenumberofpatientsthatcanbeseen,whichwerefertoasthe\ndaily capacity. For example, a Monday and Wednesday schedule with a\nDataextraction\ndailycapacityof8patientsmeansthattheprovidercanseeupto8distinct\nThisstudy(UCLAIRB#18-000630)qualifiedforUCLAIRBexceptionstatus\npatients on Monday and 8 on Wednesday. The provider cannot see a\n(waiver of consent) due to having no direct contact with patients and patientthatisinthehospitalonagivendayifthatdaydoesnotfallinthe\nusing de-identified data. We used the Perioperative Data Warehouse patient\u2019savailabilitywindow.\ndevelopedbytheUCLADepartmentofAnesthesiologyandPerioperative Wesimulatedtheproviderasfollows.Westartedthesimulationonthe\nMedicine,whichhasbeendescribedindetailinotherreferences26,27. firstdayinthesimulationhorizonthatfallsintheprovider\u2019sschedule.We\nWe extracted de-identified data corresponding to all surgical patients looked up the set of patients such that the current day falls within the\nthatwereadmittedattheUCLARonaldReaganMedicalCenter(RRMC)in patient\u2019savailabilitywindow.Weremovedanypatientsthathavealready\n2017and2018andunderwentaprocedurewithanesthesia.Weextracted beenseenbytheprovideronanypreviousday.Fortheremainingpatients,\nde-identifieddataonthedateofadmissionanddischargeofeachpatient,\nwe used the predictive model to compute the predicted probability of\nsoastomaintaintheexactsequenceofpatientadmissionsanddischarges, readmission.Wesortthosepatientsindecreasingorderofthepredicted\nandtobeabletorealisticallysimulatetheproviders\u2019workflow.Wedefined\nreadmission probability. We assumed that the provider applies the\nthe start date of the simulation as the earliest admission date over the preventionpathwaytothepatientsin thatorder,up tothelimiton the\nextractedpatients,andtheenddateasthelatestdischargedateoverthe numberofpatientsdefinedbytheprovider\u2019sdailycapacity.Forexample,if\nextractedpatients. 10patientsareavailableandtheprovider\u2019sdailycapacityis3,theprovider\nForeachadmission,wedefineabinaryvariabletoindicatewhetherthe\nwillselectthetop3patientsinpredictedreadmissionprobability.Wethen\npatient had an emergency department readmission. We follow the prior moved on to the next day in the simulation horizon that falls in the\nworkofMi\u0161i\u0107etal.28indefininganemergencydepartmentreadmissionas provider\u2019s schedule, and repeated the procedure until reaching the last\nan event when a patient enters the hospital through the emergency dayinthesimulationhorizon.Figure2providesavisualizationofasmall\ndepartment within 30 days of their first surgical discharge and is exampleofthesimulatedworkflow.\nsubsequentlymovedtoanon-emergencydepartmentinpatientlocation.\nAnexceptionwasmadeforpatientsthatweredischargedandadmittedon\nthesamecalendardaytobetteralignwiththeCentersforMedicaidand Provider effectiveness\nMedicareServices(CMS)definition29.Weadditionallymakethisexception We assumed that the prevention pathway applied by the provider is\nas the same definition is used in Mi\u0161i\u0107 et al.28, allowing us to directly imperfect.Wedefinedtheeffectivenessconstantasanumberbetween0\nconnect our simulation model to the results of this previous work, and and 100% that determines what fraction of eventual readmissions are\nreusethemodelsfromthispriorwork. successfullymitigated.Forexample,iftheproviderselects50patientsof\nwhich5areeventuallyreadmitted,andtheeffectivenessconstantis20%,\nthentheproviderisexpectedtopreventexactly1(20%of5readmissions).\nSimulation model\nWeconsideredaconservativevalueoftheeffectivenessconstantof10%,\nThe goal of our simulation model was to accurately evaluate how an whichisuniformacrossallindexadmissiontypes,althoughintheresults\nunplanned readmission prediction model would allocate a constrained weconsideredalternatevaluesof5and20%aswell.\nprovidertopatients.Weimplementedandevaluatedoursimulationmodel\nintheRandJuliaprogramminglanguages30,31.\nPerformance metrics\nPredictive models. The primary input to our simulation model was a We defined several simulation-based patient performance metrics. We\npredictivemodel,whichtakesasinputthepatient\u2019sdataatagivenpointin definedthepatientsseen(PS)asthenumberofpatientsselectedbythe\ntime relative to their admission and outputs a predicted probability of provider for the prevention pathway. We defined the readmissions\nfuture ED-based readmission. We considered four different types of anticipated(RA)tobethenumberofpatientsthatwereselectedbythe\npredictivemodels,whichwebrieflydescribebelow.Thefirsttwomodels\nprovider and that resulted in an emergency department-based hospital\nare the lab-based and non-lab-based L1 regularized logistic regression readmission.Wedefinedtheexpectedreadmissionsprevented(ERP)asRA\nmodelsfromMi\u0161i\u0107etal.28.Thesemodelsweredevelopedusingadistinct\nmultipliedbytheeffectivenessconstant.\nsubsetofadmissionsfrom2013to2016,andwerenottrainedusinganyof Wedefinedseveralsimulation-basedmetricsquantifyingtheprovider\u2019s\nthedataextractedforthepurposeofthissimulation.Weusethetermslab- costimpact.Wedefinedthereadmissioncost(RC)tobethetotalexpected\nbased/non-lab-based to indicate whether the model uses the lab-based\ncost of those patients selected by the provider that resulted in an ER\nfeatures described in Mi\u0161i\u0107 et al.28 as predictor variables. The other two\nmodels that we considered are the HOSPITAL score32 and the LACE readmission.Tocomputethis,weuseddatafromtheHealthcareCostand\nscore33.Wereferreaderstotheaforementionedreferences28,32,33formore Utilization Project (HCUP), which provides national average readmission\ncostsfordifferentdiagnosisgroups,reportedinBaileyetal.34.Foragiven\ninformationonthevariablesusedinthesefourmodels.\npatient,welookeduptheirICD10codeinTable3ofBaileyetal.,andused\nModel availability. Each model has an associated availability window, theassociatedaveragereadmissioncost(SupplementalTable2ofBailey\netal.)asthecostofthatpatient\u2019sreadmission.Inthecasethatthepatient\nwhichisawindowoftimeduringwhichthepredictionfromthemodelis\navailable, and the patient may be selected for intervention. For the had multiple ICD10 codes, we used the highest matching average\nregularizedlogisticregressionmodels,theavailabilitywindowwasdefined readmissioncost,andinthecasethatnoneoftheICD10codesmatched,\nastheperiodfromthedayonwhichsurgeryiscompletedtothedayof wedefaultedtotheaveragereadmissioncost.Wedefinedtheexpected\ndischarge. For the lab-based model, the lab-based features are defined readmissioncostsavings(ERCS)tobethereadmissioncostmultipliedby\nusingalllabmeasurementsthathaveoccurreduptoagivenpointintime the effectiveness constant, which represents the cost savings from\nduringsurgery;thus,thepredictionfromthelab-basedregularizedlogistic applyingthepreventionpathwaytotheselectedpatients.\nPublishedinpartnershipwithSeoulNationalUniversityBundangHospital npjDigitalMedicine(2021) 98Velibor.V.Mi\u0161i\u0107etal.\n8\nFig.2 Visualizationofanexampleoftheproviderworkflowinthesimulation.Eachsquarecellisapostoperativepatientinthehospitalon\naparticularday.Withineachcell,P indicatestheithpatient,whilethenumberunderneathindicatesthepredictedreadmissionprobabilityon\ni\nthatday(e.g.,forpatient1onday1,thisis0.80).Ifthereisnosquarecell,thenthepatienthaseithernotyetcompletedsurgery,orhasbeen\ndischarged.Forexample,patient5completessurgeryonday3andisdischargedonday8;beforeday3,thepatientiseitherstillinsurgeryor\nhasnotyetbeenadmitted;afterday8,thepatientisnolongerpresentinthehospital.Theverticalrectanglesshadedinlightpurpleindicate\ndaysonwhichtheproviderselectspatients.Inthisexample,weassumethattheproviderworksondays3,5,7,and10,andhasacapacityof\ntwopatients,i.e.,theprovidercanselectatmosttwopatientstoapplytheinterventiontooneachofthosefourdays.Onagivenday,ifa\npatientisavailableforselectionandhasnotyetbeenselected,theyareshadedingreen.Onagivenday,iftheproviderselectsthem,theyare\nshadedinblue.Aftertheproviderselectsthemandappliestheinterventionpathway,thepatientisshadedingrayfortheremainingdaysof\ntheir hospital stay. As an example of the provider selection process, observe that on day 3, there are 5 postoperative patients available\n(patients 1, 2, 3, 4, and 5). The provider selects patients 1 and 2, because these two patients have the highest predicted probability of\nreadmission(0.80and0.29,respectively).Afterpatients1and2havebeenselected,theycannotbeselectedagain;fortheremainingdaysthat\nthosetwopatientsareinthehospital,theyareindicatedwiththegraysquarecells.Noticethatpatient4,whowasnotselectedonday3,is\nlaterselectedbytheprovideronday5;inthiscase,patient4wasnotamongthetoptwopatientsonday3butbecameoneofthetoptwo\npatients on day 5. On the other hand, patient 3 is discharged before day 5; thus, this patient is lost and never ends up receiving the\ninterventionpathway.Wealsonotethatonday10,theprovideronlyselectsonepatient(patient10),becausetheothertwopatients(patients\n7and9)werealreadyselectedpreviously(days5and7,respectively).Therectangularcellsontherightsummarizethefinaloutcomewith\neach patient, with regard to whether or not they were selected by the provider, and whether or not they eventually experienced an ED\nreadmission. In this example, there are a total of 7 patients who are selected, so the patients seen (PS) metric is 7. The number of\npatientsselectedandwhogetreadmittedisexactly4,sotheRAmetricis4.Assuminganeffectivenessconstantof10%,theERPmetricis\n4\u00d710%=0.40.\nWealsodefinedtheprovidercost(PC)asthetotalcompensationpaidto learning literature, and contrast them to our proposed simulation-based\ntheprovideroverthesimulationhorizon.Weassumedthattheprovideris metrics.\ncompensated at a rate of $75 per hour, and each patient seen by the\nproviderrequiresanhouroftime;thus,theweeklycompensationcanbe Areaunderthereceiver\u2013operatorcharacteristiccurve(AUROC). TheAUROC\ndeterminedfromtheprovider\u2019sscheduleanddailycapacity.Forexample, metric,alsoknownasthecstatistic,ismathematicallydefinedas\na Monday and Wednesday weekly schedule and a daily capacity of 8\ncp o ofa mt $i 7e p5n et \u00d7 ns sw 1 a6 to io=u nld $ i1t sr 2a f0n u0s r. tla hIft ee t rht e io np cw r ro ee ave sik d ely e drh \u2019s bo yhu or 2s u 5o r %sf e ,1 x t6 oca e an e cdd c2a o0 uw nhe te a fk owly rec te ho k em ,t dhp iee ffn w es rea eet ni ko cln ey AUROC\u00bc N\u00fe1 N(cid:2)X i2S\u00feX j2S(cid:2)1(cid:2) Y^ i>Y^ j(cid:3) (1)\nbetween part-time providers and full-time providers who would receive ^\nadditional compensation in the form of benefits. Our assumptions here whereY i isthepredictedprobabilityofreadmissionforpatienti,S\u00fe is\nthesetofpatientsthatarereadmitted,S(cid:2)isthesetofpatientsthatare\nweremotivatedbyareviewofsalariesfornursepractitionersattheUCLA\nRRMC. There are some costs that are not accounted for by physician tn ho etr ne ua mdm beit rte od f, pN a\u00fe tii es nt th se tn hu atm ab re er no of tpa reti ae dn mts itt th ea dt ,a ar ne dre 1a \u00f0d Y^m >it Y^te \u00ded, isN(cid:2) 1i is\nf\noversight,butthesecouldbeaddedtothemodelintheformofabaseline ^ ^ i j\nY >Y and0otherwise.TheAUROCmetricmeasuresthediscrimination\ncostthatisnotassociatedwithhourlyproviderutilization. i j\nLastly,wedefinedtheexpectednetcostsavings(ENCS)asthedifference abilityofapredictivemodel.Ithasthefollowinginterpretation:givena\nrandom patient that goes on to be readmitted and a random patient\noftheexpectedreadmissioncostsavingsandtheprovidercost.Thismetric\nwasintendedtoquantifythebenefitofusingamodelforpatientselection, whodoesnot,theAUROCmeasurestheprobabilitythatthepredictive\nmodel correctly distinguishes between the two patients, i.e., that it\nnetofthecostofimplementingthemodelthroughtheprovider.\nassigns a higher predicted probability to the patient who gets\nreadmitted than the one who does not. The AUROC metric ranges\nComparison to existing predictive anddecision-analytic from 0.5 to 1. A value of 1 corresponds to perfect discrimination; in\nmetrics other words, a patient that will be readmitted is always assigned a\nWebrieflyreviewexistingmetricsforquantifyingpredictiveanddecision- higher risk score than one that will not be readmitted. On the other\nhand,avalueof0.5correspondstodiscriminationthatis\u201casgoodasa\nanalyticperformancethathavebeenproposedinthehealthcaremachine\nnpjDigitalMedicine(2021) 98 PublishedinpartnershipwithSeoulNationalUniversityBundangHospitalVelibor.V.Mi\u0161i\u0107etal.\n9\nrandom\u201d;stateddifferently,givenapatientthatisreadmittedandone categories,sothateachpatientisgiventheirowncategory.Inthiscase,v for\ni\nthatisnot,ourmodelisnobetterthanifweweretopickthereadmitted patientiwillsimplybe+1ifmodelm assignsahigherriskthanmodelm ,0\n2 1\none at random from those two patients (which would result in being ifthetwomodelsassignthesamerisk,and\u22121ifmodelm assignsalower\n2\ncorrect50%ofthetime). riskthanm .ThistypeofNRIissometimesreferredtoascontinuousNRI.\n1\nCalibration plot, slope, and intercept. The AUROC metric measures the Netbenefit (NB)and decisioncurves. TheNBmetric aimsto accountfor\ndiscriminationabilityofamodel,whichisitstendencytogivehigherrisk thebenefitfrommakingacorrectpositiveprediction,whileaccountingfor\nscores to patients that get readmitted than those who do not get theharm caused by an incorrect positive prediction17. To define theNB\nreadmitted. Besides discrimination, another dimension of predictive metric,onefirstdefinesaprobabilitycutoffpforclassifyingapatientasa\nperformanceiscalibration,whichishowwelldoesthepredictedprobability readmission.Onethencalculatesthenetbenefitas\nof readmission correspond to the actual probability of readmission.\nTP FP p\nCalibrationistypicallyassessedusingacalibrationplot,whichisconstructed NB\u00bc (cid:2) \u00b4 (6)\nN N \u00f01(cid:2)p\u00de\nasfollows.Onecomputesthepredictedprobabilityofreadmissionforeach\npatient,andbucketsthepatientsbytheirpredictedprobabilitiesaccording whereTPisthenumberoftruepositiveswhenoneusesthecutoffofp,FP\nto the deciles of their predicted probabilities. For each of the deciles isthenumberoffalsepositivesfromthesamecutoff,andp=\u00f01(cid:2)p\u00deisthe\ni\u00bc1;\u00bc;10, one computes the average predicted probability of read- exchangeratebetweentruepositivesandfalsepositives(i.e.,thevalueofa\n(cid:2)m Qis 1s ;i Qo ^n 1(cid:3) ;Q \u00bci, ;a \u00f0n Qd 10t ;h Q^e 10e \u00dem op nir aica scl ap ttr eo rb pab loi tli .ty IdeQ^ ai l. lyO ,Qne st hh oe un ldp blo ets clot sh ee topa Q^ir is mfal es te r- ip co issi tt hiv ae tr te hl eat civ ue tot fo fpa tt hr au te isp uo ss eit dive to). cA lak se siy fyfe pa at tu iere nto sf asth te run ee at nb de fn ae lsfi et\n,\ni\nandsotheresultingcalibrationcurveshouldbeascloseaspossibletothey positivesisthesamecutoffusedindefiningtheexchangerate.\n=xline(correspondingtoa45degreelinethroughtheorigin). Usingtheconceptofnetbenefit,onecanplotadecisioncurve,whichis\nIn addition to the calibration plot, one can compute two calibration a plot of net benefit against the chosen cutoff p17. One can use the\nmetrics, which are the calibration slope and intercept15. Given predicted decisioncurveofasinglemodeltofindcutoffvaluesforwhichthemodel\n^\nprobabilitiesforeachdatapointY i,oneassumesthatthelog-oddsofthe leadstomoregoodthanharm.Onecanalsocomparethedecisioncurves\nactualbinaryoutcomesarealinearfunctionofthelog-oddsofthepredicted ofmultiplemodelstoidentifywhichmodelsperformbetterthanothers.\nprobabilitiesproducedbythemodel,i.e.,\n^ Comparison with our proposed approach. The metrics and methods of\np Y\nlog 1(cid:2)i p i\u00bc\u03b1\u00fe\u03b2log 1(cid:2)i Y^ (2) e siv ma ulu laa tt ii oo nn -bt ah sa et dw me eth ria cv se thd ai tsc wu ess he ad vehe pr re opa or se edqu ei at re lied rif infer te hn ist sf er co tm iont .h Ine\ni\nwhere p is the actual probability for observation i, \u03b1 is the calibration particular,withregardtoAUROC,calibrationslopeandintercept,IDIand\ni\nintercept and \u03b2 is the calibration slope. The values of \u03b1 and \u03b2 can be NRI,wenotethatthesemetricsarestaticmetrics.Inparticular,thereisno\nestimatedusinglogisticregression.Ideally,\u03b1shouldbeascloseaspossible notionoftimeinthesemetrics:theyarecomputedwithoutaccountingfor\ntozero,and\u03b2ascloseaspossibleto1.Avalueof\u03b1thatisbelow0indicates whenpatientscompletesurgeryandwhentheyaredischarged.Theyalso\nthatthepredictivemodelsystematicallyoverestimatesthetrueprobability do not account for the capacity constraint of the provider and the\nofreadmission,whileavalueabove0indicatesthatthepredictivemodel provider\u2019sschedule.Ourmetrics,incontrast,arecomputedviaasimulation\nsystematicallyunderestimatesthetrueprobability15.Similarly,avalueof\u03b2 thatincorporatestheprovider\u2019sscheduleandthetimeswhenthepatients\nthatisgreaterthan1indicatesthatthepredictedprobabilitiesdonotvary complete surgery and are discharged. Lastly, it is also clear that these\nsufficiently;avalueof\u03b2between0and1suggeststhatthereistoomuch existing predictive metrics (AUROC, calibration slope and intercept, NRI\nvariationinthepredictedprobabilities,andavalueof\u03b2below0suggests and IDI) do not incorporate costs and the financial benefit of correctly\nthat the predicted probabilities vary in the wrong direction (low intervening on true positive patients, which are incorporated in our\nprobabilitiesarepredictedforobservationsforwhichtheactualprobability financialmetrics(ERC,ERCS,PC,andENCSdefinedearlier).\nishigh,andviceversa)15. Withregardtothenetbenefitmetric(andtheassociatedconceptofa\ndecisioncurve),wenotethatthismetriciscloserinspirittothesimulation-\nbasedmetricsthatweproposehere,asthenetbenefitmetricdoesattempt\nIntegrateddiscriminationimprovement(IDI). TheIDImetricisameasureof\nthe improvement in discrimination of one model over another model16. toincorporatea\u201ccost\u201d,whichisthecostofinterveningonafalse-positive\nThe two models could be from the same model class but vary in their patient.However,theNBmetricstilldiffersfromourmetricsbecause,like\npredictors, or they could be from different model classes. To define it theothermetricswehavehighlightedabove,itstilldoesnotincorporate\nformally,wefirstdefinethediscriminationslopeofamodelas how patients become available over time and the limit of how many\nP P patientscanbeseenbytheprovider.Inaddition,themeaningofacostis\n^ ^\nDS\u00bc i N2S \u00fe\u00feY i(cid:2) i N2S (cid:2)(cid:2)Y i (3) od uiff rer se imnt ub lae tit ow nee mn oo du er l,si tm heula ct oio sn m ario sd ee sl fa ron mdth the ene at ctb ue an le fifi nt afr na cm iae lw coor sk t; oin\nt f\npayingtheprovidertoworkandseepatients,whereas,inthenetbenefit\nIn words, the discrimination slope is the difference in the average\nmetric,thecostarisesfromincorrectlytreatingafalse-positivepatient.\npredictedprobabilitybetweenpatients that are readmitted and patients\nWe also note that the use of a cutoff probability for deciding who\nthatarenot.Giventwomodelsm 1 andm 2,theIDIisdefinedas receives an intervention is not efficient when patients are seen by the\nIDI m1;m2 \u00bcDS m1(cid:2)DS m2 (4) p pr ao tiv ei nd te sr ad cy cn oa rdm inic gal tl oy. aT po au rtn icd ue lr as rta scn hd ew duh ly e, ,wsu ip thp aos fie xeth da ct at ph ae cip tyro invid tee rr ms see os\nf\nwhichisjustthedifferencebetweenthediscriminationslopesofthetwo\nhowmanypatientscanbeseenoneachdayinthatschedule,andonlysees\nmodels.\npatientswhoseriskexceedsagivencutoff.Onsomedays,wemayhavefew\nornopatientswhoexceedthecutoff,andsomepatientswhoarebelowthe\nNetreclassificationimprovement(NRI). TheNRImetric,similarlytotheIDI cutoff; if the provider has capacity remaining after seeing those patients\nmetric, measures the improvement in the discrimination of one model who exceed the cutoff, then it makes sense for the provider to also see\noveranothermodel.FollowingPencinaetal.16,supposethatwedividethe\nsomeofthosepatientswhoarebelowthecutoff.Onotherdays,wemay\nunitrange[0,1]intoacollectionofintervalsorcategories(forexample,the havemorepatientswhoexceedthecutoffthantheprovider\u2019scapacity;in\nintervals[0,0.1],(0.1,0.2],(0.2,0.3],(0.3,1.0]).Giventwomodelsm 1andm 2, this case, within the group of patients who exceed the cutoff, it makes\nweletv i foreachpatientibe+1ifmodelm 2placespatientiinahigher sensetoprioritizepatientsaccordingtotheirpredictedrisk.\nriskintervalthanmodelm 1,\u22121ifmodelm 2placespatientiinalowerrisk Inaddition,anotherdifferencebetweentheNBmetricandourproposed\ninterval than model m 1, and otherwise 0 if the two models place the metricsisthatthecostinthenetbenefitframeworkisdefinedimplicitly\npatientinth Pesameris Pkinterval.TheNRIisthendefinedas. using the chosen cutoff probability, and is a variable cost that depends\nNRI m1;m2 \u00bc i N2S \u00fe\u00fev i(cid:2) i N2S (cid:2)(cid:2)v i (5) l c Ein u Nte Coa Sfr f )l ,y p tro hon ebt a ch b oe i sli tn tyu i. sm In db ece fior nno etf r dafa s etls , xe i pn- lp io co iu ts lri yti cv ioe nstp d-ba ot a li le s aen rdt ts emt rh mea t srt i .ca Isr nis (Ee aR df Cr do , im tE iR oC nth S ,e , tP hc C eh ,o cas one sdn\nt\nSome papers refer to this NRI as the categorical NRI, as it is defined with doesnotdependonthenumberofpatientsthattheproviderintervenes\nrespecttoafinitecollectionofcategories.AspecialcaseofthisNRImetricis on:thecostonlydependsonthelengthofthesimulationperiod,asthis\nobtained when one considers an infinite continuous collection of risk defineshowmuchcompensationisrequiredfortheprovider.\nPublishedinpartnershipwithSeoulNationalUniversityBundangHospital npjDigitalMedicine(2021) 98Velibor.V.Mi\u0161i\u0107etal.\n10\nGeneralization to other applications 3. Beam,A.L., Manrai,A.K.&Ghassemi,M.Challenges tothereproducibility of\nThe methodology we have described is explicitly formulated in terms of machinelearningmodelsinhealthcare.JAMA323,305(2020).\nsurgical readmissions. However, as noted earlier in the Introduction, our 4. Mi\u0161i\u0107, V. V. & Perakis, G. Data analytics in operations management: a review.\nmethodology is applicable whenever (1) there is a constrained resource Manuf.Serv.Oper.Manag.22,158\u2013169(2020).\nthatcanprevent(perfectlyorimperfectly)sometypeofevent;(2)onehas 5. Bertsimas,D.,O\u2019Hair,A.K.&Pulleyblank,W.R.TheAnalyticsEdge(DynamicIdeas\naccesstoamodelforpredictingagivenpatient\u2019sriskofthatevent;and(3) LLC,2016).\noneisinterestedinquantifyingthebenefitofallocatingtheresourceusing 6. Lee,C.K.,Hofer,I.,Gabel,E.,Baldi,P.&Cannesson,M.Developmentandvali-\nthe model. Our simulation model is most valuable in the early stages of dation of a deep neural network model for prediction of postoperative in-\nmodel development, when one is comparing different models before hospitalmortality.Anesthesiology129,649\u2013662(2018).\ncommitting to a single model, and reliance on classical metrics, such as 7. Bertsimas,D.etal.Developmentandvalidationofanoptimizedpredictionof\nAUROC,canbemisleading.Themainfactorpresentinourmodelthatisnot mortality for candidates awaiting liver transplantation. Am. J. Transplant. 19,\nfullyaddressedbythemodelandwouldneedtobeaddressedseparatelyis 1109\u20131118(2019).\nthe design of the prevention resource and the prevention pathway. As 8. Rajkomar,A.etal.Scalableandaccuratedeeplearningwithelectronichealth\ndiscussed in the \u201cLimitations\u201d subsection of the \u201cDiscussion\u201d section, our records.npjDigit.Med.1,18(2018).\nsimulationmodeldoesnotprescribehowtheresourceisabletoprevent 9. Cabitza, F., Rasoini, R. & Gensini, G. F. Unintended consequences of machine\ntheevent.Asthissimulationmodelisintendedtobeasteppingstoneto learninginmedicine.JAMA318,517(2017).\nactualreal-timeimplementationofamachinelearning-guidedprevention 10. Peterson,E.D.Machinelearning,predictiveanalytics,andclinicalpractice:can\nresource, it is necessary to have already designed such a prevention thepastinformthepresent?JAMA322,2283\u20132284(2019).\nresourceandapathwayforeffectingtheprevention.Oursimulationmodel 11. Ahmed,Z.,Mohamed,K.,Zeeshan,S.&Dong,X.Artificialintelligencewithmulti-\ntakestheexistenceofsuchapreventionresourceasagiven,anddoesnot functional machine learning platform development for better healthcare and\ndirectlysuggestwhatthepreventionresourceandpathwayshouldbe.To precisionmedicine.Database2020,baaa010(2020).\nprovidemoreclarityonthegeneralityofthesimulationmodel,wedescribe 12. Ling,C.X.,Huang,J.&Zhang,H.AUC:ABetterMeasurethanAccuracyinCom-\ntwootherpotentialapplicationsofthistypeofapproachbelow. paringLearningAlgorithms329\u2013341(Springer,2003).\n13. Rosset,S.ModelselectionviatheAUC.InTwenty-FirstInternationalConferenceon\nMachineLearning-ICML\u20190489(ACMPress,2004).\nSepsis prevention. Suppose that a hospital forms an intervention team\nthatcanbeappliedtoanintensivecareunit(ICU)patienttoreducethat 14. Huang,J.&Ling,C.X.UsingAUCandaccuracyinevaluatinglearningalgorithms.\npatient\u2019sriskofsepsis,andthattheteamcanonlybeappliedtoatmost IEEETrans.Knowl.DataEng.17,299\u2013310(2005).\none patient each day. Suppose that the hospital develops a machine 15. Miller,M.E.,Langefeld,C.D.,Tierney,W.M.,Hui,S.L.&McDonald,C.J.Validation\nofprobabilisticpredictions.Med.Decis.Mak.13,49\u201357(1993).\nlearningmodelforpredictingtheriskofsepsisforICUpatients;anumber\nofexamplesofsuchmodelsexistintheliterature35\u201337.Ifoneallocatesthe 16. Pencina,M.J.,D\u2019Agostino,R.B.Sr.,D\u2019Agostino,R.B.Jr.&Vasan,R.S.Evaluating\ninterventionteamdailytothepatientreceivingthehighestriskprediction theaddedpredictiveabilityofanewmarker:fromareaundertheROCcurveto\nreclassificationandbeyond.Stat.Med.27,157\u2013172(2008).\nfrom the model, how many cases of sepsis would be hypothetically\npreventedandwhatwouldbethecorrespondingnetbenefitintermsof 17. Vickers,A.J.,VanCalster,B.&Steyerberg,E.W.Netbenefitapproachestothe\ncost?Oursimulationmodelcanbeusedtoanswerthesequestions. evaluationofpredictionmodels,molecularmarkers,anddiagnostictests.BMJ352,\ni6(2016).\n18. Kelly,C.J.,Karthikesalingam,A.,Suleyman,M.,Corrado,G.&King,D.Keychal-\nAcute kidney injury prevention. Suppose that a hospital develops a\nmachine learning model for predicting a patient\u2019s risk of acute kidney lengesfordeliveringclinicalimpactwithartificialintelligence.BMCMed.17,195\ninjury38,39andusesthemachinelearningmodeltoautomaticallydirecta (2019).\nspecialist nephrologist or rapid response team to the five highest risk 19. Yu,K.-H.&Kohane,I.S.Framingthechallengesofartificialintelligenceinmed-\nicine.BMJQual.Saf.28,238\u2013241(2019).\npatients each day40. How many cases of AKI could be hypothetically\n20. Sendak, M. P. et al. A path for translation of machine learning products into\nprevented through such an ML-guided consultation scheme and how\nhealthcaredelivery.EMJInnov.10,19\u2013172(2020).\nmuchofareductionincosttothehospitalcanbeachievedthroughsuch\n21. Wiens, J. et al. Do no harm: a roadmap for responsible machine learning for\nprevention, net of the cost of the specialist/rapid response team? Our\nhealthcare.Nat.Med.25,1337\u20131340(2019).\nsimulationmodelcanagainbeusedtoanswerthesequestions.\n22. Burns,M.L.&Kheterpal,S.Machinelearningcomesofage:localimpactversus\nnationalgeneralizability.Anesthesiology132,939\u2013941(2020).\nReporting summary 23. Holzinger,A.,Biemann,C.,Pattichis,C.S.&Kell,D.B.Whatdoweneedtobuild\nFurtherinformationonresearchdesignisavailableintheNatureResearch explainableAIsystemsforthemedicaldomain?Preprintathttps://arxiv.org/abs/\nReportingSummarylinkedtothisarticle. 1712.09923(2017).\n24. FDA. US FDA Artificial Intelligence and Machine Learning Discussion Paper\n(2021).\nDATAAVAILABILITY 25. Leppin,A.L.etal.Preventing30-dayhospitalreadmissions:asystematicreview\nand meta-analysis of randomized trials. JAMA Intern. Med. 174, 1095\u20131107\nThedatasetsgeneratedduringand/oranalyzedduringthecurrentstudyarenot\n(2014).\npublicly available due to institutional restrictions on data sharing and privacy\n26. Hofer, I. S., Gabel, E., Pfeffer, M., Mahbouba, M. & Mahajan, A. A systematic\nconcerns.However,thedataareavailablefromtheauthorsonreasonablerequest.\napproach to creation of a perioperative data warehouse. Anesth. Analg. 122,\n1880\u20131884(2016).\n27. Epstein,R.H.,Hofer,I.S.,Salari,V.&Gabel,E.SuccessfulImplementationofa\nCODEAVAILABILITY\nPerioperativeDataWarehouseUsingAnotherHospital\u2019sPublishedSpecification\nThecodeusedtoperformoursimulationswaswritteninJuliaversion1.5.031andis From Epic\u2019s Electronic Health Record System. Anesth. Analg. 132, 465\u2013474\navailableathttp://github.com/vvmisic/finsim-code/. (2020).\n28. Mi\u0161i\u0107,V.V.,Gabel,E.,Hofer,I.,Rajaram,K.&Mahajan,A.Machinelearningpre-\nReceived:17June2020; Accepted: 21May2021; dictionofpostoperativeemergencydepartmenthospitalreadmission.Anesthe-\nsiology132,968\u2013980(2020).\n29. CMS.InMedicareClaimsProcessingManual(CenterforMedicareandMedicaid\nServices,2021).\n30. RCoreTeam.R:ALanguageandEnvironmentforStatisticalComputing(2019).\n31. Bezanson,J.,Edelman,A.,Karpinski,S.&Shah,V.B.Julia:afreshapproachto\nnumericalcomputing.SIAMRev.59,65\u201398(2017).\nREFERENCES 32. Donz\u00e9,J.,Aujesky,D.,Williams,D.&Schnipper,J.L.Potentiallyavoidable30-\n1. Waring,J.,Lindvall,C.&Umeton,R.Automatedmachinelearning:reviewofthe day hospital readmissions in medical patients. JAMA Intern. Med. 173, 632\nstate-of-the-artandopportunitiesforhealthcare.Artif.Intell.Med.104,101822(2020). (2013).\n2. Shameer, K., Johnson, K. W., Glicksberg, B. S., Dudley, J. T. & Sengupta, P. P. 33. vanWalraven,C.etal.Derivationandvalidationofanindextopredictearlydeath\nMachine learning in cardiovascular medicine: are we there yet? Heart 104, orunplannedreadmissionafterdischargefromhospitaltothecommunity.CMAJ\n1156\u20131164(2018). 182,551\u2013557(2010).\nnpjDigitalMedicine(2021) 98 PublishedinpartnershipwithSeoulNationalUniversityBundangHospitalVelibor.V.Mi\u0161i\u0107etal.\n11\n34. Bailey,M.K.,Weiss,A.J.,Barrett,M.L.&Jiang,H.J.StatisticalBrief#248:Char- COMPETINGINTERESTS\nacteristicsof30-DayAll-CauseHospitalReadmissions,2010-2016.http://www.hcup- E.G.isthefounderandSecretaryofClarityHealthcareAnalyticsInc.,acompanythat\nus.ahrq.gov/reports/statbriefs/sb248-Hospital-Readmissions-2010-2016.jsp assists hospitals with extracting and using data from their EMRs. The remaining\n(AgencyforHealthcareResearchandQuality,2019). authorsdeclarenocompetinginterests.\n35. Futoma,J.etal.Animprovedmulti-outputgaussianprocessrnnwithreal-time\nvalidation for early sepsis detection. In Machine Learning for Healthcare Con-\nference243\u2013254(2017). ADDITIONAL INFORMATION\n36. Nemati,S.etal.Aninterpretablemachinelearningmodelforaccurateprediction\nSupplementary information The online version contains supplementary material\nofsepsisintheICU.Crit.CareMed.46,547(2018).\navailableathttps://doi.org/10.1038/s41746-021-00468-7.\n37. Henry,K.E.,Hager,D.N.,Pronovost,P.J.&Saria,S.Atargetedreal-timeearly\nwarning score (TREWScore) for septic shock. Sci. Transl. Med. 7,\n299ra122\u2013299ra122(2015). CorrespondenceandrequestsformaterialsshouldbeaddressedtoV.V.M.\n38. Toma\u0161ev,N.etal.Aclinicallyapplicableapproachtocontinuouspredictionof\nfutureacutekidneyinjury.Nature572,116\u2013119(2019). Reprintsandpermissioninformationisavailableathttp://www.nature.com/reprints\n39. Davis,S.E.,Lasko,T.A.,Chen,G.,Siew,E.D.&Matheny,M.E.Calibrationdriftin\nPublisher\u2019snoteSpringerNatureremainsneutralwithregardtojurisdictionalclaims\nregression and machine learning models for acute kidney injury. J. Am. Med.\ninpublishedmapsandinstitutionalaffiliations.\nInform.Assoc.24,1052\u20131061(2017).\n40. Park,S.etal.Impactofelectronicacutekidneyinjury(AKI)alertswithautomated\nnephrologistconsultationondetectionandseverityofAKI:aqualityimprove-\nmentstudy.Am.J.KidneyDis.71,9\u201319(2018).\nOpen Access This article is licensed under a Creative Commons\n41. Kundu,S.,Aulchenko,Y.S.,vanDuijn,C.M.&Janssens,A.C.J.W.PredictABEL:an\nAttribution 4.0 International License, which permits use, sharing,\nR package for the assessment of risk prediction models. Eur. J.Epidemiol. 26,\n261\u2013264(2011). adaptation,distributionandreproductioninanymediumorformat,aslongasyougive\nappropriatecredittotheoriginalauthor(s)andthesource,providealinktotheCreative\nCommonslicense,andindicateifchangesweremade.Theimagesorotherthirdparty\nmaterial in this article are included in the article\u2019s Creative Commons license, unless\nindicatedotherwiseinacreditlinetothematerial.Ifmaterialisnotincludedinthearticle\u2019s\nCreativeCommonslicenseandyourintendeduseisnotpermittedbystatutoryregulationor\nAUTHORCONTRIBUTIONS exceedsthepermitteduse,youwillneedtoobtainpermissiondirectlyfromthecopyright\nholder.Toviewacopyofthislicense,visithttp://creativecommons.org/licenses/by/4.0/.\nV.V.M.contributedtostudydesign,dataanalysis,andmanuscriptpreparation.K.R.\ncontributedtostudydesignandmanuscriptpreparation.E.G.contributedtostudy\ndesign,dataextraction,andmanuscriptpreparation. \u00a9TheAuthor(s)2021\nPublishedinpartnershipwithSeoulNationalUniversityBundangHospital npjDigitalMedicine(2021) 98"
}