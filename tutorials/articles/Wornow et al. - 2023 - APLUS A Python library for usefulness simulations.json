{
    "article_id": "0ed9ecb2-f37f-436f-8425-9d7288b1c4ec",
    "extracted_text": "Author\nPublished in final edited form as:\nJ Biomed Inform. 2023 March ; 139: 104319. doi:10.1016/j.jbi.2023.104319.\nManuscript\nAPLUS: A Python library for usefulness simulations of machine\nlearning models in healthcare\nMichael Wornowa,*, Elsie Gyang Rossb,c, Alison Callahanb,1, Nigam H. Shahb,d,e,f,1\naDepartment of Computer Science, Stanford University, Stanford, CA, USA\nbCenter for Biomedical Informatics Research, Stanford University School of Medicine, Stanford,\nCA, USA\nAuthor\ncDepartment of Surgery, Division of Vascular Surgery, Stanford University School of Medicine,\nStanford, CA, USA\nManuscript\ndDepartment of Medicine, Stanford University School of Medicine, Stanford, CA, USA\neClinical Excellence Research Center, Stanford University School of Medicine, Stanford, CA, USA\nfTechnology and Digital Services, Stanford Health Care, Palo Alto, CA, USA\nAbstract\nDespite the creation of thousands of machine learning (ML) models, the promise of improving\npatient care with ML remains largely unrealized. Adoption into clinical practice is lagging, in\nlarge part due to disconnects between how ML practitioners evaluate models and what is required\nAuthor\nfor their successful integration into care delivery. Models are just one component of care delivery\nworkflows whose constraints determine clinicians\u2019 abilities to act on models\u2019 outputs. However,\nmethods to evaluate the usefulness of models in the context of their corresponding workflows Manuscript\nare currently limited. To bridge this gap we developed APLUS, a reusable framework for\nquantitatively assessing via simulation the utility gained from integrating a model into a clinical\nworkflow. We describe the APLUS simulation engine and workflow specification language, and\napply it to evaluate a novel ML-based screening pathway for detecting peripheral artery disease at\nStanford Health Care.\nKeywords\nMachine learning; Utility; Model deployment; Discrete-event simulation; Clinical workflows;\nAuthor\nUsefulness assessment\nManuscript\nThis is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n*Corresponding authors: mwornow@stanford.edu (M. Wornow).\n1co-senior authors.\nDeclaration of Competing Interest\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nCRediT authorship contribution statement\nMichael Wornow: Conceptualization, Methodology, Software. Elsie Gyang Ross: Data curation, Supervision. Alison Callahan:\nConceptualization, Methodology, Supervision. Nigam H. Shah: Conceptualization, Methodology, Supervision.Wornow et al. Page 2\n1. Introduction\nAuthor\nWhile the development of models in healthcare via machine learning (ML) continues at\na breakneck pace [1\u20134], deployment of models into practice remains limited [5\u20137]. As an\nexample, a recent survey found evidence of adoption for only a small fraction of over Manuscript\n250,000 published clinical risk prediction systems [8]. The primary reason for limited\nadoption is that models are only one component of the care delivery workflows that they are\ndesigned to improve [6,9\u201311]. Healthcare workflows are complex: successful care delivery\noften depends on the coordination of several providers across multiple departments to\nexecute many steps in highly specific, context-dependent sequences [12]. Their ability\nto execute is impacted by resource constraints, patient needs, and existing protocols\n[13,14]. Asking a clinician to incorporate information output by an ML model may require\nredesigning workflows or altering behavior. For example, a workflow mapping exercise done\nat Kaiser to understand how an early deterioration model fit into care delivery identified\nAuthor\n44 different states and 6 different departments needed to act upon the model\u2019s output [10],\nwhile process mapping at Stanford Hospital revealed 21 steps and 7 handoffs necessary to\nput an ML model for advance care planning into practice [15]. Manuscript\nTraditional metrics for evaluating ML models are insufficient for assessing their usefulness\nin guiding care [6,7,16\u201319]. Popular metrics such as Area Under the Receiver Operating\nCharacteristic curve (AUROC), F1 Score, and Accuracy ignore a workflow\u2019s capacity\nconstraints as well as the variable costs of misprediction. For example, the cost of\nadministering an unnecessary cancer screening (false positive) may be much lower than\nthe cost of failing to detect cancer (false negative). The outcomes that actually matter to\na health system \u2013 namely, cost-effectiveness and net benefit to patients \u2013 are not captured\nby these traditional metrics [6,7,16\u201319]. An approach which does take these factors into\nAuthor\naccount is utility analysis. There has been extensive research on using utility as a north-star\nmetric for predictive models. Vickers et al. 2006 introduced decision curves as an improved\nmethod for comparing predictive models [17], and Baker et al. 2009 introduced a similar\nManuscript\nconcept called relative utility [20]. While these utility-based metrics complement traditional\nML metrics like AUROC, they still suffer from an important underlying limitation \u2013 namely,\nthey estimate theoretical rather than achievable utility gained from using a model [6,9,14].\nThey assume that every model prediction gets acted upon, and thus ignore the structure and\nconstraints of the relevant workflow (e.g. budget, staffing, data acquisition delays, human\nerror, etc.) [6,9,21].\nBecause deploying an ML model into a healthcare setting has a cost in terms of time, money,\nand potential risk of harm to patients [14,22,23], proactively assessing the overall benefit of\nusing a model to guide care is essential for two reasons. First, deciding which models to Author\nimplement \u2013 even a simple tool can cost hundreds of thousands of dollars [23]. Second, for\ndeciding how best to integrate a chosen model\u2019s output into a care delivery workflow.\nManuscript\nIn the remainder of this paper, we refer to this process of conducting a utility-based analysis\nof a model within the context of the workflow into which it will be deployed as a usefulness\nassessment. In essence, a usefulness assessment aims to quantitatively answer the question:\nIf I use this ML model to guide this workflow, will the benefits outweigh the costs, and\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 3\nby how much? Prior work has used simulations to identify potential changes to clinical\nworkflows that can reduce operational inefficiencies in healthcare settings [24\u201328], but Author\nefforts to quantify the usefulness of model-guided care delivery workflows are limited.\nExisting approaches rely on bespoke simulation pipelines that are specific to the workflow\nManuscript\nbeing evaluated and are not designed for reuse [9,29\u201333].\nTo bridge this gap in tooling, we have developed APLUS, a simulation framework\nfor systematically conducting usefulness assessments of ML models in triggering care\nmanagement workflows. Of the various aspects of clinical decision support systems that\nmerit study, our framework is specifically focused on measuring the integration and\nevaluation of ML models prior to their deployment [11]. Our framework can simulate any\nworkflow that can be represented as a set of states and transitions, and includes the ability to\nincorporate individual-level utility analyses [29], heterogeneous treatment effects and costs,\nand preferential allocation of shared resources. We make APLUS available as a Python\nAuthor\nlibrary and demonstrate its use by conducting a usefulness assessment of a classification\nmodel for identifying patients with peripheral artery disease (PAD) [34].\nManuscript\n2. Methods\nIn this section, we describe the design of APLUS including its simulation engine, workflow\nspecification language, and the analyses it supports. The code for APLUS is available on\nGithub at https://github.com/som-shahlab/aplus.\n2.1. Simulation engine\nInspired by prior work to develop simulation tools for patient flow [25\u201328], we take a\npatient-centric, discrete-event approach to workflow simulation. Specifically, we developed\nAuthor\na synchronous, time-driven discrete-state simulator [35]. Given a set of patients and a\nworkflow (defined via the specification language described below in 2.1.2 Workflow\nSpecification Language), the simulation engine progresses all patients through the workflow\nManuscript\nand tracks their state history, transitions taken, and utilities achieved. We model each\nindividual patient\u2019s journey through the workflow as an ordered sequence of states occurring\nover a set of evenly spaced time steps, where cycles are permitted. Each state and transition\nis associated with a non-negative integer duration which represents the number of time steps\nthat it takes to complete. Within a single timestep, states are unordered, with two exceptions:\n(i) states that are dependent on the completion of previous states will always occur after\nthose prior dependencies have been completed, (ii) patients who reach a state which has at\nleast one transition that depends on a shared resource will be sequentially processed based\non an end-user-defined function that preferentially ranks patients for access to that shared\nAuthor\nresource. This enables modeling of a limited resource that is allocated based on a patient\u2019s\npredicted risk \u2013 e.g. Stage IV cancer patients getting first access to a novel therapy.\nManuscript\nWhen there are multiple possible transitions that a patient can take from a state, there\nare two possible ways that a specific transition can be selected by the simulation engine.\nTransitions can be associated with (i) a probability of occurrence or (ii) a conditional\nexpression. For transitions associated with a probability (e.g. patients have a 30 % chance\nof going down the \u201chigh-risk\u201d pathway and a 70 % chance of going down the \u201clow-risk\u201d\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 4\npathway), the simulation engine samples from the set of possible transitions proportional\nto their specified probabilities. This is best suited for workflows in which one might have Author\na rough sense of what proportion of patients go down each treatment pathway, but it is\ndifficult to articulate more precise criteria. For transitions associated with a conditional\nManuscript\nexpression (e.g. patients with attributes X and Y are \u201chigh-risk\u201d, patients with attributes\nW and Z are \u201clow-risk\u201d) the simulation engine sequentially evaluates each expression\nand selects the transition whose expression evaluates to True, thus allowing APLUS to\ngeneralize to essentially any situation that can be expressed as a set of Boolean conditions.\nFor example, heterogeneous treatment effects and costs can be simulated via a conditional\nexpression predicated on a patient-level property variable, as defined below in 2.2 Workflow\nSpecification Language. A gene therapy workflow might take a patient to the \u201ctreatment\nsucceeded\u201d state if the patient has a specific gene mutation, and otherwise send the\npatient to a \u201ctreatment failed\u201d state. Patients belonging to different \u201ccontext groups\u201d\n(i.e. subpopulations) can have different workflow trajectories applied to them via these Author\nconditional transitions [36]. Transitions can also be conditioned on the availability of\nsystem-level resources to model the impact of resource constraints.\nManuscript\nThe simulation engine is written in Python. The outputs that it generates are a set of Python\ndictionaries and objects which are subsequently analyzed via the methods described below\nin 2.1.3 Utility Analyses.\n2.2. Workflow specification language\nBuilding on previous work on mapping healthcare workflows [9,37,38] and clinical\nguidelines [39\u201341] into machine comprehensible representations [42], we created a\nlightweight workflow specification language for APLUS. Our language was designed with\nthree key points of differentiation in mind. First, our target end user is an informatician\nAuthor\nwith intermediate programming skills, rather than a clinician or business operations analyst.\nThus, we prioritized the ability to easily modify workflows programmatically over concerns\nlike user interfaces or integrations with clinical ontologies. Second, we wanted to enable Manuscript\nfast iteration over many workflow variations. Thus, we prioritized simplicity and speed\nof writing over support for edge cases that would add significant complexity. Third, we\nwanted APLUS\u2019s core simulation engine to support a broad range of workflows without\nmodification. Thus, our language prioritizes expressive flexibility. A detailed schema is\navailable at the APLUS GitHub repository.\nWe represent a care delivery workflow as a state machine consisting of a set of states and\ntransitions. We take a \u201cpatient-centric\u201d view, i.e. the state machine represents the journey of\nan individual patient through the workflow. We represent our specification language using\nAuthor\nYAML, a popular markup language that aims to be both human- and machinereadable [43].\nThus, the only dependency for creating a workflow specification is a basic text editor (e.g.\nvim, TextEdit, Notepad). Concretely, an APLUS workflow specification has three sections:\nManuscript\nmetadata, variables, and states.\nThe metadata section contains information needed to initialize the simulation. This includes\nthe name of the workflow, the locations of relevant files that will be imported (e.g. a CSV\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 5\ncontaining predictions from a relevant model), and column mappings for tabular data (e.g.\nwhich column in a CSV corresponds to patient IDs). Author\nThe variables section contains a list of all of the variables that will be used in the\nsimulation. Variables can be referenced in the definition of any state or transition in the Manuscript\nstates section. There are four main types of variables that are currently supported: (1)\nSimulation-Level Variables are tracked by the simulation engine itself and measure the\nprogression of time within the simulation. Examples include: the number of timesteps the\nsimulation has already run, the duration of time that a patient has spent in the hospital, the\nduration of time that a patient has before being discharged, etc. (2) Patient-Level Properties\nrepresent unique, individual-level properties associated with each patient. Examples include\nmodel predictions, ground truth labels, age, stage of cancer, etc. (3) System-Level Resources\nrepresent attributes of the overall system (e.g. hospital, department, outpatient clinic, etc.).\nAs such, they are shared across all patients. When one patient depletes a system-level\nAuthor\nresource, that change will be reflected across all other patients. Examples include budget,\nMRI availability, specialist capacity, etc. (4) Constants are variables in the purest sense \u2013\nthey are not directly associated with the overall workflow or any individual patient. They can Manuscript\nbe any primitive Python type (integer, float, string, or boolean) or basic Python data structure\n(list, dict, set). Examples include: 0.98, [1\u20133], True, etc.\nThe states section describes the structure of the workflow. We represent a workflow as a\nstate machine, and thus the states section contains a list of all states in the workflow, as\nwell as the possible transitions between them. There are three types of states: start, end,\nand intermediate. All patients begin their journey at the same start state, pass through 0 +\nintermediate state(s) over the course of the simulation, and finish their journey at one of the\nend states. A patient moves between states via transitions. If only one transition is specified\nAuthor\nfor a state, then every patient who reaches that state will take that transition. If a state has\nmultiple transitions, however, the simulator will follow whatever rules were specified for\neach transition to decide which one to take.\nManuscript\nIn order to measure the outcomes from executing a workflow, utilities must be associated\nwith each state and/or transition. Utilities are grouped by their unit of measurement,\nso multiple distinct types of utilities can be simultaneously tracked. This permits the\nassessment of a model across a wide range of performance indicators of interest to a health\nsystem, including time-related (e.g. length-of-stay), clinical (e.g. patient outcomes), financial\n(e.g. monetary cost), or resource-related (e. g. staff utilization) [44]. Utility values can also\nbe conditioned on arbitrary expressions. This allows for conducting individual-level utility\nanalyses by including patient-level variables in a utility\u2019s associated conditional expression\n[29]. This tends to be the most difficult step of specifying a workflow, as the end user must Author\nobtain these utility values via literature review, expert interviews, or financial modeling [32].\nFor example, a usefulness assessment that had been previously conducted on an advance\nManuscript\ncare planning workflow derived utilities from a previously published randomized controlled\ntrial [9].\nIn order to represent the temporal nature of workflows, durations of time can be associated\nwith each state and transition. These durations represent the number of discrete time steps\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 6\nthat a patient waits after reaching a state or taking a transition. For example, if we have a\nworkflow where patients stay in a hospital for multiple days post-surgery and are evaluated Author\nonce a day for additional treatment, then we could set up two states named \u201crest\u201d and\n\u201cevaluation\u201d, where the transition between \u201crest\u201d and \u201cevaluation\u201d takes 0 timesteps since\nManuscript\nthey occur on the same day, but the transition between \u201cevaluation\u201d and \u201crest\u201d takes 1\ntimestep since after evaluating a patient, we progress to the next day.\nIn order to model how resource constraints change over time, resource deltas can be\nassociated with each state and transition. A resource delta encodes how a system-level\nresource changes after a transition is taken. For example, a transition which directs a patient\nto an MRI machine might have a resource delta of \u201c\u22121\u201d for the resource \u201cMRI capacity.\u201d\nThis setting allows for fine-grained control over the depletion and augmentation of shared\nresources which influence the delivery of care.\nAuthor\n2.3. Utility analyses\nAPLUS conducts three categories of analyses to assess a model\u2019s usefulness: predictive\nperformance, theoretical utility, and achievable utility under workflow constraints. The Manuscript\nanalysis outputs automatically generated by APLUS (with examples in Appendix A) are\nas follows:\n1. Plots which summarize the model\u2019s predictive performance, including Receiver\nOperating Characteristic (ROC) curve, Precision-Recall curve, Calibration curve,\nWork v. PPV/TPR/FPR, and Model Cutoff Threshold v. PPV/TPR/Work. Work\nis defined as the proportion of model predictions that are positive, PPV is\nthe model\u2019s positive predictive value, TPR is the true positive rate, FPR is\nthe false positive rate, and model cutoff threshold is the value above which\nwe consider a model\u2019s probabilistic output to be a positive prediction. These Author\nstandard measurements of model performance gauge the ability of an ML model\nto make accurate predictions.\nManuscript\n2. Plots which summarize the model\u2019s theoretical utility, i.e. the outcomes\nachieved by following the model\u2019s predictions after weighting them by their\ncorresponding utilities. The plots that we generate include ROC curve with\nutility indifference curves, Precision-Recall curve with utility indifference\ncurves, Decision curve, Relative Utility curve, PPV v. Mean Utility Per Patient,\nModel Cutoff Threshold v. Mean Utility Per Patient, and Work v. Mean Utility\nPer Patient. A decision curve is a plot of a model\u2019s net benefit across different\nrisk thresholds. Net benefit is defined as the difference between the TPR and\nFPR of a model, where the FPR is translated onto the same scale as the TPR\nAuthor\nvia an \u201cexchange rate\u201d which depends on the relative utility of true v. false\npositives [17]. As an analogy, one can imagine net benefit being the \u201cprofit\u201d\nof using a model, where the \u201crevenue\u201d is generated in one currency (true Manuscript\npositives) while the \u201ccosts\u201d are generated in another currency (false positives),\nand thus there is an intermediary step in which the currency of costs (false\npositives) are \u201cexchanged\u201d into the currency of revenue (true positives) [45].\nRisk threshold is defined as the cutoff value above which a model\u2019s probabilistic\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 7\noutput is considered to be a positive prediction. Thus, a decision curve allows a\nreader to quickly compare different models\u2019 expected utilities across various risk Author\nthresholds. A slight variation is the relative utility curve [46]. The relative utility\nof a model at a given risk threshold is the maximum net benefit achieved by the\nManuscript\nmodel divided by the net benefit achieved by a perfect classifier [46].\n3. Plots which summarize the model\u2019s achievable utility within the context of\nits workflow, i.e. how much utility we expect the model to achieve given the\nconstraints of the overall care delivery pathway that it impacts. The plots that we\ngenerate include Model Cutoff Threshold v. Mean Achieved Utility Per Patient,\nMean Achieved Utility Per Patient v. Optimistic Baseline, and Mean Achieved\nUtility Per Patient under Workflow Variant #1 v. Mean Achieved Utility Per\nPatient under Workflow Variant #2. We define Optimistic Baseline as the case\nin which all of a model\u2019s predictions get acted upon. Note that these evaluation\nAuthor\nmetrics integrate information about the entirety of the workflow, and can also\nrelax some of the assumptions made in generating the theoretical utility plots.\nFor example, decision curve analyses assume that utilities are uniform across\nManuscript\nall patients, an assumption that does not need to be enforced within an APLUS\nsimulation [19]. Additionally, theoretical utility analyses ignore the possibility\nof downstream constraints turning positive predictions into true/false negatives,\nor turning negative predictions into true/false positives (e.g. if alternative tests\nor clinicians determine that a patient originally assigned a negative prediction\nshould be treated as a positive case).\n3. Application of APLUS to risk models for PAD screening\nIn this section, we present a case study of conducting a novel usefulness assessment via Author\nAPLUS of ML models for the early detection of PAD.\n3.1. Clinical background of PAD Manuscript\nTo demonstrate APLUS in action, we applied it to a novel usefulness assessment scenario\n\u2013 identifying patients with PAD via a state-of-the-art ML model for further screening [34]\n\u2013 and show that our framework can be used to select the optimal model and workflow\ncombination to maximize the model\u2019s usefulness to patients. PAD is a chronic condition\nwhich occurs when the arteries in a patient\u2019s limbs are constricted by atherosclerosis,\nthereby reducing blood flow [47]. A total of 8\u201312 million people in the US have PAD [48],\ncosting the US healthcare system over $21 billion annually [49]. Left untreated, PAD is\nassociated with a higher risk of mortality, serious cardiovascular events, and lower quality\nof life [50]. Despite these risks, PAD is often missed by healthcare providers. Roughly half Author\nof all PAD patients are asymptomatic [50], and one study showed that even when a previous\nPAD diagnosis was documented in a patient\u2019s medical record, only 49 % of primary care\nManuscript\nphysicians were aware [48]. A broad suite of treatment options is available to patients\nsuffering from PAD, ranging from lifestyle changes to drugs to surgery [50,51]. The earlier\nthat a patient is diagnosed, the better the chances of preventing disease progression and thus\navoiding the need for costlier interventions [32,34]. The low-risk and non-invasive ankle-\nbrachial index (ABI) is the primary test used to diagnose PAD today [32,52]. However,\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 8\nthe most recent guidance from the US Preventive Services Task Force cited \u201cinadequate\nevidence\u201d on the usefulness of population-wide ABI testing to identify asymptomatic PAD Author\npatients who might benefit from further treatment [53].\n3.2. Previously developed ML models for PAD screening Manuscript\nGhanzouri et al. 2022 developed three ML models to classify patients for PAD based\nsolely on EHR data: a deep learning model, a random forest, and a logistic regression\nwith respective AUROCs of 0.96, 0.91 and 0.81 [34]. Each model assigns a probabilistic\nrisk score to each visiting patient which indicates their likelihood of having PAD. Patients\nwith risk scores above a certain threshold (chosen to be 0.5 in their study) are classified\nas having PAD and recommended for follow-up ABI testing [34]. We extend this prior\nwork by conducting a usefulness assessment on incorporating a PAD classification model\u2019s\npredictions into clinical decision making at Stanford Health Care.\nAuthor\n3.3. Specification of the PAD screening workflows\nTo apply APLUS to this use case, we first mapped out the states, transitions, and utilities\nManuscript\nof possible model-guided PAD screening workflows. Based on interviews with practitioners\n(chiefly, co-author ER, who is a practicing vascular surgeon), we identified two workflows\nto consider: (1) a nurse-driven workflow which assumes the existence of a centralized team\nof nurses reviewing the PAD model\u2019s predictions for all patients visiting their clinic each\nday; and (2) a doctor-driven workflow which assumes that the PAD model\u2019s predictions\nappear as a real-time alert in a patient\u2019s EHR during their visit to the clinic. These interviews\nyielded natural language descriptions of both workflows, which we then translated into the\nAPLUS specification language.\nIn both the nurse-driven and doctor-driven workflows, our experiments assumed that there Author\nwere 3 possible end outcomes for patients: \u201cUntreated\u201d, \u201cMedication\u201d, or \u201cSurgery.\u201d These\nroughly capture the spectrum of treatment options available for patients with PAD \u2013 either\nthe patient\u2019s visit is concluded without treatment, the patient is prescribed medication Manuscript\nto reduce the risk of cardiovascular disease, or the patient undergoes a procedure like\nangioplasty or bypass [50,51]. We assume that patients who end up in the \u201cSurgery\u201d state\nhave also been given medication prior to their procedures. The utility of each of these\noutcomes depends on the ground truth PAD status for a specific patient. For example,\n\u201cUntreated\u201d is the best option for patients without PAD but has the largest cost for patients\nwith PAD. \u201cMedication\u201d is the ideal outcome for patients with moderate PAD but is\nundesirable for patients without PAD. \u201cSurgery\u201d is the costliest outcome for all patients,\nbut the relatively best option for patients with severe PAD. We combined clinician input\nwith utility estimates from Itoga et al. 2018 to define the utilities associated with the Author\nend outcomes of each workflow in terms of a multiplier on remaining years living to\nreflect quality-adjustment on lifespan [32]. Given that a healthy patient with no PAD has a\nbaseline utility of 1, we used the following relative utilities for various outcomes: 0.95 for Manuscript\npatients without PAD who are prescribed medication, 0.9 for patients with PAD who are\nprescribed medication, 0.85 for patients with moderate PAD not prescribed medication, 0.7\nfor patients without PAD who undergo surgery, 0.68 for patients who have severe PAD and\nundergo surgery, and 0.6 for patients with severe PAD who do not undergo surgery [32].\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 9\nThe corresponding YAML workflow specification files are available in the APLUS Github\nrepository. Author\nIn both workflows, we also assume the existence of a cardiovascular specialist who can\nevaluate patients after they are referred by a doctor or nurse. We assume that the specialist Manuscript\nhas a set capacity for how many patients she can see per day. However, once a patient\nreaches the specialist, we assume that the specialist makes the optimal treatment decision for\nthat patient. Thus, prioritizing which patients use up the limited capacity of the specialist is\nthe key driver of our simulated workflow\u2019s achieved utility.\nThe doctor-driven workflow assumes that model predictions will appear as an alert within\nthe EHR of a patient during their visit to the clinic. If the attending physician notices this\nalert, she can choose to either ignore the alert or act on it. We assume that physicians\nignore alerts at random. If a physician decides to act on an alert, she will either administer\nAuthor\ntreatment herself or refer the patient to a specialist. The main constraints on this workflow\nare the probability that the attending physician reads the alert (previous studies have shown\nthat up to 96 % of alerts are overridden [54\u201356]) and the specialist\u2019s schedule.\nManuscript\nThe nurse-driven workflow assumes the existence of a centralized team of nurses tasked\nwith reviewing the predictions of the PAD model for each patient who visits the clinic\non a given day. Based on these predictions, the nursing staff decides which patients to\ndirectly refer to the specialist, thus cutting out any intermediate steps with a non-specialist\nphysician. The main constraints on this workflow are the capacity of the nursing staff\nand the specialist\u2019s schedule. We assume that the nursing staff does not suffer from alert\nfatigue, i.e. they will not randomly ignore predictions from the PAD model. This is an\nassumption we have made in this study, and we acknowledge that nurses might suffer from\nalert fatigue as well. This is an example of a potentially significant assumption which can Author\nbe easily changed within APLUS by anyone interested in replicating our experiments under\na different set of nurse-driven workflow constraints (e.g. including a probability that nurses\nManuscript\nignore alerts).\nOne important distinction between these two workflows is that the nurse-driven workflow\nis centralized whereas the doctor-driven workflow is decentralized. In other words, the\nnurse-driven workflow batches together all model predictions for each day before patients\nare chosen for follow-up, while in the doctor-driven workflow each doctor decides whether\nto act on a PAD alert immediately upon receipt of the alert independently from the decisions\nof other doctors. Thus, specific to the nurse-driven workflow with a daily capacity of K,\nwe consider two possible strategies that the nurses can leverage for processing this batch of\npredictions: (1) ranked screening, in which the nursing staff follows up with the K patients\nAuthor\nwith the highest PAD risk scores; or (2) thresholded screening, in which a random subset\nof K patients are selected from the batch of predictions whose predicted PAD risk score\nexceeds some cutoff threshold. Manuscript\n3.4. Simulation parameters for the PAD workflows\nWe acquired a dataset of 4,452 patients (henceforth referred to as the \u201cdataset\u201d) who had\nboth ground-truth labels of PAD diagnosis and risk score predictions from all three ML\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 10\nmodels developed in Ghanzouri et al. 2022 [34]. This data was directly sourced from\nthe authors of Ghanzouri et al. 2022 [34], who had previously run their models on this Author\ncohort of patients at Stanford Hospital and who share two co-authors with this paper (ER\nand NHS). For each combination of model/workflow that we evaluated, we simulated 500\nManuscript\nconsecutive days of patient visits. For each simulated day, the number of visiting patients\nwas determined by randomly sampling from a Poisson distribution with a mean of 35 (this\ndistribution was chosen to reflect historical patterns of the rate at which patients who would\ntrigger the PAD model visit Stanford clinics). Then, given the number of patients visiting\non a given day, we randomly sampled (with replacement) that number of patients from\nour dataset. The same set of sampled patients was used across all simulations to ensure\ncomparability of results.\nBased on clinician interviews and a literature review, we identified the following parameters\nfor our simulation. First, we assumed that patients with PAD have an ABI sampled from\nAuthor\na normal distribution with a mean of 0.65 and standard deviation of 0.15, while patients\nwithout PAD have an ABI sampled from a normal distribution with a mean of 1.09 and\nstandard deviation of 0.11 [47]. We used an ABI of 0.90 as the cutoff threshold between Manuscript\nPAD and no PAD [57]. Our simulated ABI test showed roughly 95 % sensitivity and 95\n% specificity on our simulated patients, which is consistent with previous estimates for the\naccuracy of an ABI test [32,47,52]. To simulate the increased risk of serious complications\nfrom untreated PAD, we assumed that if a PAD patient saw a specialist, then they would\nneed surgery only if their ABI score was < 0.45, whereas a PAD patient who did not see\na specialist would eventually require surgery if their ABI score was < 0.55. The overall\nproportion of simulated patients with an ABI score < 0.45 was 9 %, while the proportion\nof simulated patients with an ABI score < 0.55 was 26 %, which emulates the fact that\nroughly 7 % of patients with PAD will require surgical intervention [48], and that 26 % of\nAuthor\npatients with symptomatic PAD should eventually progress to needing some form of surgery\nto manage their PAD [32]. We also assumed that an ABI score > 0.8 was moderate enough\nto be treated by a non-specialist physician, but that a score < 0.8 must be referred to a Manuscript\nspecialist [57].\nFor the doctor-driven workflow specifically, we assumed that only patients who have a\nPAD risk score \u2265 0.5 will generate an alert, which is consistent with the threshold used\nin Ghanzouri et al. 2022 (42). The representations of these two workflows in the APLUS\nspecification language, as well as visualizations of their states and transitions, can be found\nin our Github repository: https://github.com/som-shahlab/aplus. The specifications can be\nviewed in any basic text editor, but for ease of visualization we recreate diagrams of the\nnurse-driven and doctor-driven workflows in Fig. 1.\nAuthor\nWe evaluated the doctor-driven and nurse-driven workflows across ranges of possible values\nfor two constraints: (1) nurse capacity for the nurse-driven workflow and (2) probability\nManuscript\nthat a PAD alert is read for the doctor-driven workflow. Nurse capacity is defined as the\ntotal number of patients per day that the nursing team can follow-up with for an ABI\ntest. Probability that a PAD alert is read (also referred to as probability alert is read or\nsimply alert fatigue) is the chance that a doctor acts on an alert generated when a patient is\nclassified by a model as having PAD.\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 11\n3.5. Usefulness assessment of the PAD workflows\nAuthor\nWe evaluated each PAD model\u2019s utility relative to three baselines: Treat None, where the\nmodel simply predicts a PAD risk score of 0 for all patients; Treat All, where the model\npredicts a PAD risk score of 1 for all patients; and Optimistic, where there were no workflow\nManuscript\nconstraints or resource limits on model predictions. Concretely, we measured each model\u2019s\nexpected utility achieved per patient above the Treat None baseline as a percentage of the\nutility achieved under the Optimistic scenario. In other words, we measured how much\nof the total possible utility gained from using a model was actually achieved under each\nworkflow\u2019s constraints. The Treat None baseline should therefore always have a relative\nachieved utility of 0 %, while all models should have a utility value of 100 % in the\nOptimistic setting (as all patients are simply sent to the specialist for screening). For clarity,\nwe do not show the Treat None baseline in any of the following plots, as it is always trivially\nset to 0 %.\nAuthor\n4. Results\nIn this section, we summarize the results of conducting our APLUS usefulness assessment Manuscript\non the doctor-driven and nurse-driven PAD workflows under realistic capacity constraints.\n4.1. Simulating unlimited downstream specialist capacity\nFor our first set of experiments, we assumed that the capacity of the downstream specialist\nwas infinite to isolate the impact of nurse capacity on the nurse-driven workflow and alert\nfatigue on the doctor-driven workflow.\n(A) Nurse-driven workflow: As detailed below, APLUS revealed that the choice of\nmodel for a nurse-driven workflow mattered only under the medium- and high-capacity\nAuthor\nsettings, and certain screening strategies. This was because the three models had similar\ntop-K precision (i.e. they were all able to identify the most obvious PAD cases), and thus\nin low-capacity settings where only the top few model predictions could be acted upon, the Manuscript\nchoice of model does not matter. In higher-resource settings, however, the deep learning\nmodel offered a significant boost in utility.\nTo determine this, we first used APLUS to evaluate the thresholded screening strategy, in\nwhich a random subset of K patients is selected from the batch of patients whose predicted\nPAD risk score exceeds the cutoff threshold. As shown in Fig. 2a, the deep learning ML\nmodel (purple line) achieves the highest expected utility per patient across all treatment\nstrategies under this thresholded screening regime. A nursing staff which leverages the deep\nlearning model to prioritize patients can achieve roughly 50 % of the total possible utility\nAuthor\nunder the optimistic scenario with a screening capacity of only 5 patients per day, and\nalmost 80 % of the total possible utility with a screening capacity of 10 patients per day. As\nthe nursing capacity increases, we see the difference between the relative utility achieved by\nManuscript\nthe deep learning model and the random forest model (blue line) increasing from 4 absolute\npercentage points under a capacity of 3 patients/day to 13 percentage points under a capacity\nof 6 patients/day.\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 12\nWe observe similar overall trends in Fig. 2b when simulating a ranked screening strategy in\nwhich the nursing staff follows up with the K patients with the highest PAD risk scores. Author\nHowever, APLUS reveals slightly different results in low resource settings (i.e. nurse\ncapacity < 4). In a low-capacity setting, all three ML models achieve relatively similar\nManuscript\nutilities. This makes sense, as nurses are only able to act on each model\u2019s most confident\nprediction under this constrained setting, and thus the achieved utility of the model depends\nonly on the accuracy of its top-3 highest scoring predictions, rather than its overall predictive\nperformance across all patients. The difference between the achieved utility of the deep\nlearning model (purple line) and random forest model (blue line) is smaller than it is under\nthe thresholded screening strategy, ranging from only 1 absolute percentage points with a\ncapacity of 3 patients/day to 7 percentage points with a capacity of 6 patients/day.\nUsing APLUS, we can now conclude the following: For workflows with a nurse capacity\n\u2265 4, a deep-learning-guided ranked screening approach, rather than a thresholded screening\nAuthor\napproach, yields the highest expected achieved utility. For workflows with a nurse capacity\n< 4, however, the results are mixed \u2013 the three ML models do not appear to be differentiated\nfrom a utility standpoint, and the ranked screening approach does not yield a consistently Manuscript\nhigher utility than the thresholded screening approach. This indicates that there are enough\nadditional steps and constraints in the nurse-driven workflow under these low resource\nsettings that the choice of model or patient prioritization does not make a tangible\ndifference.\nAs an additional experiment, we were curious about the impact of varying the cutoff\nthreshold used for each of the ML models on their expected achievable utility. The results\nshown in Fig. 3 provide a hint for why the deep learning model showed superior utility in\nour previous analysis \u2013 the probability distribution it learned more accurately reflected the\nAuthor\nbinary prediction task it was given than either of the other two models. This can be seen in\nthe significantly sharper, immediate jump in expected utility that the deep learning model\n(far left) experiences as its cutoff threshold increases from 0 compared to the more gradual\nManuscript\nslope in the utility curves of the random forest (middle) and logistic regression (far right).\nThis reflects the better calibration and accuracy of the deep learning model, as its predictions\nhave a highly bimodal distribution clustered around 0 and 1 (of its total set of probabilistic\npredictions, 63 % are < 0.01 while 16 % are > 0.99). As shown in Fig. 3, this makes the deep\nlearning model highly sensitive to increases in cutoff threshold around 0 and 1, whereas the\nmore dispersed probability distributions learned by the random forest and logistic regression\ncause their cutoff thresholds to have a more gradual impact on their achieved utilities. By\nmaking these types of differences more readily apparent, APLUS can help to debug and\ncompare models. Author\n(B) Doctor-driven workflow: We simulated a doctor-driven workflow in which we\nassumed that every physician who sees a patient with a predicted risk score \u2265 0.5 would Manuscript\nreceive an EHR alert recommending follow-up [34]. The one exception was the Treat\nAll case, in which an alert was automatically sent for all patients. Again, this simulation\nassumed that the capacity of the specialist was infinite to isolate the impact of the\nprobability that an alert is read. We see in Fig. 4 that a strategy of Treat All (red line)\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 13\nuniformly generates the highest expected utility under this doctor-driven workflow, with\nalerts triggered by the deep learning model (purple line) coming in a distant second. This Author\nwas expected, as our workflow assumed that patients assessed by a specialist would always\nhave better outcomes than patients who were not. Thus, any increase in the number of alerts\nManuscript\nthat we simulated would also increase the number of patients referred to a specialist, and\nsince the specialist had unlimited capacity under this experimental setting, this would always\nresult in better outcomes for patients.\n4.2. Simulating finite downstream specialist capacity\nA more realistic setting is a downstream cardiovascular specialist with finite capacity. Thus,\nwe repeated the above experiments under the assumption that our specialist could see a\nmaximum of 2 referred patients per day.\n(A) Nurse-driven workflow: Though the deep learning model still shows the strongest Author\nperformance of all treatment strategies across all nursing capacity levels, its achievable\nutility caps out at a nurse capacity of 3 patients per day. This is because the downstream\nManuscript\nspecialist\u2019s capacity is the limiting factor capping the achievable utility of the model. Thus,\na policymaker deciding how to staff a nursing-driven workflow in which the downstream\ncardiovascular specialist can only see 2 patients per day could feel comfortable with staffing\nto a capacity of 3 patients per day, regardless of how many patients might be flagged by the\nmodel.\nWe see this clearly in Fig. 5. The thresholded screening strategy is shown in the far-left\npanel and shows that the deep learning model (purple line) yields high improvements\nin utility over alternative treatment strategies. However, this difference quickly becomes\nnegligible at higher nurse capacity levels (e.g. > 3 patients per day). This is the opposite\nAuthor\nof the conclusion that we had previously reached under an unlimited capacity setting.\nThere, we found that the deep learning model\u2019s advantage grew as the nursing team\u2019s\ncapacity grew. This example illustrates the importance of factoring in capacity constraints Manuscript\nwhen evaluating models, as they can substantially distort the incremental gain of increasing\nresource allocation to act on a model\u2019s output.\nThis result is replicated under the ranked screening strategy in Fig. 5b which shows that\nthe ML models do not exhibit substantially different achieved utility across potential nurse\ncapacities. This is similar to the parity across models that we observed in our analysis of\nunlimited specialist capacity when considering low-resource nursing teams.\n(B) Doctor-driven workflow: In the case of the doctor-driven workflow, Fig. 6 shows\nstrong differentiation across all three ML models in the limited specialist setting. However, Author\nwe now see that the deep learning model (purple line) achieves a higher relative utility than\nthe Treat All (red line) strategy once the probability of an alert being read is above 0.4. This\nManuscript\ncan be explained as follows. When doctors are more likely to respond to alerts, more patients\nwill be referred to the specialist, but the specialist will have to turn people away because of\nthe specialist\u2019s limited capacity (set to 2 patients/day in this experiment). Thus, ensuring that\nwe only send patients who are likely to have PAD to the specialist becomes more important\nas doctors become increasingly willing to act on the alerts they see (and thus exceed the\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 14\ncapacity of the specialist to handle referrals). The accuracy of the model therefore has more\ninfluence on the workflow\u2019s utility as the probability increases that a doctor reads an alert. Author\nIn our case, the deep learning model had the best predictive performance, hence the utility\nwhen using this model was greatest at higher levels of alert responsiveness.\nManuscript\n4.3. Comparing the two proposed integration pathways\nGiven the results of our first two analyses, which demonstrated the superiority of the deep\nlearning model, the next question we aimed to answer was which of the two workflows\noffered the optimal deployment strategy for the model. For this experiment, we focused on\nquantifying the trade-off between nursing capacity and alert fatigue, as this was the primary\nquestion that came up in our conversations with clinicians. Our guiding question was as\nfollows: How many patients would a staff of nurses need to screen per day to have the\nnurse-driven workflow yield the same expected utility as a doctor-driven workflow with a\ngiven level of alert fatigue? Author\nTo answer this question, we used APLUS to measure the deep learning model\u2019s achieved\nutility under different nurse capacities (using a ranked screening strategy) and compared Manuscript\nthis against the utility achieved under the doctor-driven workflow as the probability that\ndoctors read alerts increased (where an alert was generated if the predicted probability of\nPAD for a patient was \u2265 0.5). We then subtracted the latter from the former to calculate the\nincremental gain in achievable utility that could be expected by adopting the nurse-driven\nworkflow at that capacity level over a doctor-driven workflow at that alert fatigue level.\nWe plotted the results as a heatmap in Fig. 7, under the assumption that the downstream\nspecialist can see 5 patients per day. The y-axis is the nursing capacity that a cell\u2019s utility\nvalue is calculated at, while the x-axis shows the level of alert fatigue in the doctor-driven\nworkflow that corresponding to that cell\u2019s measurement. Red squares (positive numbers)\nAuthor\nindicate that the nurse-driven workflow is expected to yield more utility at that capacity\nlevel than the corresponding doctor-driven workflow, while blue squares (negative numbers)\nindicate that the doctor driven workflow should be preferred. This allows a policymaker to Manuscript\nquickly determine what nurse capacity is required for the nurse-driven workflow to have\na greater expected utility than the doctor-driven workflow with a given level of PAD alert\nacceptance.\n5. Discussion\nWe have demonstrated the use of APLUS to quantify the relative utility achieved by using\nthe three ML models proposed in Ghanzouri et al. 2022 to drive two possible workflows for\nPAD screening [34]. In our evaluation of these models, we factored in the consequences of\nAuthor\nthe downstream patient care decisions that they enabled, as well as the impact of resource\nconstraints on their usefulness. Our results affirm that the deep learning model results in\nthe largest gains in relative utility compared to the other proposed models under certain\nManuscript\nworkflow settings, but we also found that constraints on the capacity of a cardiovascular\nspecialist to handle referrals can create a hard bound on the achievable utility of a model-\nguided screening workflow. Our simulations also helped to quantify the trade-off between\nchoosing a nurse-driven v. doctor-driven workflow for model implementation. Specifically,\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 15\nwe investigated the impact of screening capacity on the nurse-driven workflow and the\nimpact of alert fatigue on the doctor-driven workflow. We identified the conditions under Author\nwhich one of these integration pathways yields higher expected utilities, and thus greater\nusefulness, via a sensitivity analysis of nurse capacity and alert fatigue.\nManuscript\nThe plots generated by APLUS can help to quantify the expected utility that can be achieved\nby deploying each of the three ML models into one of the two workflows considered. This\nyielded insights that were not readily apparent by simply looking at ROC curves \u2013 namely,\nthe parity across models in low-capacity settings for the nurse-driven workflow (in which\ncase the simpler/more explainable model, logistic regression, may be preferable given its\nidentical performance to the opaque deep learning model), the significantly higher utility\nunlocked by the deep learning model in the doctor-driven workflow (especially at lower\nlevels of alert fatigue), and the incremental value of using one workflow over the other at\ndifferent capacity levels as shown in the heatmap of Fig. 7. All of these utility-based results\nAuthor\ndepended on simulating both the model and its surrounding workflow via APLUS, and could\nnot have been determined via traditional ML evaluation metrics like AUROC.\nManuscript\nThough we focus on PAD screening as a case study, APLUS generalizes to a broader range\nof ML models and decision support situations. APLUS can simulate any scenario involving\na machine learning model to classify or predict patient state that meets two conditions: (i)\nthe clinical workflow of interest can be represented as a set of states and transitions (i.e. a\nfinite state machine), and (ii) there is a cohort of patients with their associated ML model\noutputs available as input to APLUS.\nTo assess the generalizability of our approach, we also conducted APLUS usefulness\nsimulations for another care delivery workflow that had been previously evaluated in terms\nof clinical utility \u2013 a model-guided workflow for prioritizing advance care planning (ACP) Author\nconsultations [9]. We were able to successfully replicate the results of the previous study\u2019s\nanalyses (for brevity, results are not shown; the code is available at the APLUS GitHub\nManuscript\nrepository). The primary differences between the ACP use case and the PAD screening use\ncase are that the model used for the ACP use case is a mortality prediction model (its output\nis a probability of death 3\u201312 months in the future for an individual patient) rather than a\nPAD classification model, and the clinical workflow triggered by model output differs as\nwell (see [9] for ACP workflow details).\nThe process for conducting the ACP usefulness assessment was very similar to that for the\nPAD screening use case. The ACP workflow, defined via interviews with clinicians, was\nconverted into the APLUS specification language, utilities were sourced from the palliative\ncare literature, and a dataset of patients and their associated mortality model predictions\nAuthor\nwere acquired from the original authors of the ACP machine learning model. Any one\nof these steps may present a challenge when simulating other workflows using APLUS:\nutilities can be hard to quantify, workflow steps may be vaguely defined, or the ML model Manuscript\nmay be inaccessible to researchers.\nAdditionally, we performed custom analyses to evaluate the effect of alternative care\npathways for ACP on the model\u2019s usefulness. For an informatician applying APLUS to their\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 16\nown model-guided workflow, writing such custom analyses will likely require additional\neffort. This is both a limitation and strength of APLUS \u2013 APLUS can support arbitrary Author\ndownstream analyses because it imposes minimal assumptions on the workflow simulations,\nwhich comes with the tradeoff of requiring additional custom tuning to support specific use\nManuscript\ncases.\nThe design of our workflow specification language has several key strengths. First, unlike\nprior usefulness assessments which directly hardcoded the structure of the workflow\ninto the simulation and analysis logic [9,31,32], APLUS explicitly separates the act\nof defining a workflow from the act of simulating and analyzing it. This facilitates\ninteroperability between APLUS and existing analysis pipelines with minimal code\nrefactoring. Additionally, APLUS logs the entirety of each patient\u2019s trajectory such that\ntheir journeys can be reconstructed post hoc, which enables arbitrary downstream analyses\nwithout the need to re-run the simulation. Second, our simulation engine can simulate\nAuthor\nvirtually any workflow that can be represented as a set of states and transitions in which\na patient can only ever be in one state concurrently. The ability to specify many types of\ntransition conditions allows APLUS to handle intricate branching logic within a workflow. Manuscript\nThird, because APLUS simulates the actual trajectories of patients rather than simply\nrepresenting patients/workflows as a set of equations, it supports more complex workflows,\nbranching conditions, and probability distributions than would be feasible to analytically\ndescribe. Fourth, our specification language takes advantage of the expressivity that YAML\nenables, including human-readability, straightforward version control, minimal dependencies\n(e.g. just a text editor), and a simple interface for programmatically manipulating the settings\nof a workflow (e.g. any of the existing YAML-parsing libraries for Python). This allows an\nanalyst to quickly generate and test many workflow variations. Fourth, APLUS supports the\nspecification of variables (i.e. utilities and resource constraints) that have varying units. This\nAuthor\nenables the end user to simultaneously measure how quantities such as QALYs and dollars\nare impacted by a model-guided workflow, rather than having to conduct two separate\nsimulations focused on each unit of measurement in isolation. This also helps to increase Manuscript\nreplicability by forcing the end user to be precise in how they define the properties of\ntheir workflow. Beyond the software that we have developed, another unique aspect of our\nwork is our close collaboration with a clinician partner (ER) who was directly involved in\ndesigning the workflows that we simulated. This ensured that our experiments accurately\nreflected real-world care delivery pathways.\nThere are several limitations of our work. First, an analyst is still required to do the\npreliminary legwork of mapping out a care workflow\u2019s steps. This is an inherently non-\ntechnical task which can represent a large bottleneck in the usefulness assessment process,\nAuthor\nbecause it requires either actively scheduling and conducting interviews with stakeholders\nand operations personnel [9], or automated process mining of clinical pathway patterns\nwhich requires detailed analysis of previously collected data/event logs [44,58]. To aid in\nManuscript\nthis process mapping step, we recommend that analysts partner with members of a hospital\u2019s\noperations, quality improvement, or business management offices. Second, because our\nframework makes minimal assumptions about the structure/length/design of the workflow\nbeing simulated, the end user must specify many detailed aspects of their workflow.\nWhile we do provide plausible defaults, this task can be time consuming for complex\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 17\nworkflows and requires the informatician to decide on the proper level of simplification.\nThird, our choice of time-driven discrete-event-based simulation suffers from several known Author\ncomputational inefficiencies, such as a lack of parallelizability and inefficient modeling of\nlonger time intervals, that can be addressed through further algorithmic refinement of our\nManuscript\ncore simulation engine [35,59,60].\nOur framework is general enough to assess a wide range of workflows, and we look forward\nto demonstrating the full depth of APLUS\u2019s capabilities by applying it to future usefulness\nassessments. By reducing the need to write bespoke scripts, our work can help to accelerate\nand systematize this process across health systems. At Stanford Health Care, this work is\none component of a larger effort to develop a delivery science for fair, useful, and reliable\nadoption of models to guide care management workflows [61]. Accomplishing this goal\nrequires automated methods such as APLUS.\nAuthor\nMore broadly, we aim for our research to be useful for both large health systems with\nexpertise in ML deployments, as well as health systems without much experience. At\nacademic medical centers which aim to conduct usefulness assessments across dozens of\nManuscript\nmodels, our tool can help to systematize and scale this evaluation process [61,62]. For\nhealth systems with more limited resources and less expertise in ML, the availability of\nan off-the-shelf tool like APLUS which can readily quantify the benefit of investing in the\nimplementation of an ML model may encourage funding its development.\n6. Conclusion\nWe have presented APLUS, a framework for conducting usefulness assessments of ML\nmodels that considers the properties of the care workflows that they drive. We applied\nAPLUS to yield implementation insights for a new care delivery workflow \u2013 the early Author\nscreening of PAD via machine learning. More broadly, our simulation engine can assist in\nunderstanding the usefulness of model-guided care prior to committing to deployment. We\nhope that our library enables other researchers to study a wide range of workflows, thereby Manuscript\ndeepening our field\u2019s understanding of the impact of workflow constraints on ML model\nusefulness in healthcare.\nAcknowledgments\nMW is supported by an NSF Graduate Research Fellowship. NHS, AC, and MW acknowledge support from the\nGordon and Betty Moore Foundation and Stanford Medicine for this research.\nAuthor\nManuscript\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 18\nAppendix A\nAuthor\nManuscript\nA set of plots that are automatically generated by APLUS to measure a 12-month all-cause\nmortality model\u2019s predictive performance and theoretical utility.\nAuthor\nReferences\n[1]. Topol EJ, High-performance medicine: the convergence of human and artificial intelligence, Nat.\nManuscript\nMed 25 (2019) 44\u201356, 10.1038/s41591-018-0300-7. [PubMed: 30617339]\n[2]. Lee D, Yoon SN, Application of Artificial Intelligence-Based Technologies in the Healthcare\nIndustry: Opportunities and Challenges, Int. J. Environ. Res. Public Health 18 (2021) 271,\n10.3390/ijerph18010271. [PubMed: 33401373]\n[3]. Ngiam KY, Khor IW, Big data and machine learning algorithms for health-care delivery, Lancet\nOncol. 20 (2019) e262\u2013e273, 10.1016/S1470-2045(19)30149-4. [PubMed: 31044724]\n[4]. Miotto R, Wang F, Wang S, Jiang X, Dudley JT, Deep learning for healthcare: review,\nopportunities and challenges, Brief. Bioinform 19 (2018) 1236\u20131246, 10.1093/bib/bbx044.\n[PubMed: 28481991]\n[5]. Obermeyer Z, Weinstein JN, Adoption of Artificial Intelligence and Machine Learning Is\nIncreasing, but Irrational Exuberance Remains, NEJM Catalyst. 1 (2020) CAT.19.1090. 10.1056/\nAuthor\nCAT.19.1090.\n[6]. Shah N, Making Machine Learning Models Clinically Useful, J. Am. Med. Assoc 322 (2019)\n1351, 10.1001/jama.2019.10306.\nManuscript\n[7]. Marwaha JS, Landman AB, Brat GA, Dunn T, Gordon WJ, Deploying digital health tools within\nlarge, complex health systems: key considerations for adoption and implementation, npj Digital\nMed. 5 (2022) 13, 10.1038/s41746-022-00557-1.\n[8]. Challener DW, Prokop LJ, Abu-Saleh O, The Proliferation of Reports on Clinical Scoring\nSystems: Issues About Uptake and Clinical Utility, J. Am. Med. Assoc 321 (2019) 2405\u20132406,\n10.1001/jama.2019.5284.\n[9]. Jung K, Kashyap S, Avati A, Harman S, Shaw H, Li R, Smith M, Shum K, Javitz J, Vetteth Y, Seto\nT, Bagley SC, Shah NH, A framework for making predictive models useful in practice, J. Am.\nMed. Inform. Assoc 28 (2021) 1149\u20131158, 10.1093/jamia/ocaa318. [PubMed: 33355350]\n[10]. Dummett BA, Adams C, Scruth E, Liu V, Guo M, Escobar GJ, Incorporating an Early Detection\nSystem Into Routine Clinical Practice in Two Community Hospitals, J. Hosp. Med 11 (2016)\nS25\u2013S31, 10.1002/jhm.2661. [PubMed: 27805798] Author\n[11]. Greenes RA, Bates DW, Kawamoto K, Middleton B, Osheroff J, Shahar Y, Clinical decision\nsupport models and frameworks: Seeking to address research issues underlying implementation\nsuccesses and failures, J. Biomed. Inform 78 (2018) 134\u2013143, 10.1016/j.jbi.2017.12.005. Manuscript\n[PubMed: 29246790]\n[12]. Kannampallil TG, Schauer GF, Cohen T, Patel VL, Considering complexity in healthcare\nsystems, J. Biomed. Inform 44 (2011) 943\u2013947, 10.1016/j.jbi.2011.06.006. [PubMed: 21763459]\n[13]. Seneviratne MG, Shah NH, Chu L, Bridging the implementation gap of machine learning in\nhealthcare, BMJ Innovations. 6 (2020), 10.1136/bmjinnov-2019-000359.\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 19\n[14]. Morse KE, Bagley SC, Shah NH, Estimate the hidden deployment cost of predictive models\nto improve patient care, Nat. Med 26 (2020) 18\u201319, 10.1038/s41591-019-0651-8. [PubMed:\nAuthor\n31932778]\n[15]. Li RC, Smith M, Lu J, Avati A, Wang S, Teuteberg WG, Shum K, Hong G, Seevaratnam B,\nWestphal J, Dougherty M, Rao P, Asch S, Lin S, Sharp C, Shieh L, Shah NH, Using AI to\nManuscript\nEmpower Collaborative Team Workflows: Two Implementations for Advance Care Planning and\nCare Escalation, NEJM Catalyst. 3 (n.d.) CAT.21.0457. 10.1056/CAT.21.0457.\n[16]. Baker SG, Decision Curves and Relative Utility Curves, Med. Decis. Making 39 (2019) 489\u2013490,\n10.1177/0272989X19850762. [PubMed: 31104590]\n[17]. Vickers AJ, Elkin EB, Decision Curve Analysis: A Novel Method for Evaluating Prediction\nModels, Med. Decis. Making 26 (2006) 565\u2013574, 10.1177/0272989X06295361. [PubMed:\n17099194]\n[18]. Keane PA, Topol EJ, With an eye to AI and autonomous diagnosis, NPJ Digital Med. 1 (2018)\n40, 10.1038/s41746-018-0048-y.\n[19]. Kerr KF, Brown MD, Zhu K, Janes H, Assessing the Clinical Impact of Risk Prediction Models\nWith Decision Curves: Guidance for Correct Interpretation and Appropriate Use, J. Clin. Oncol\n34 (2016) 2534\u20132540, 10.1200/JCO.2015.65.5654. [PubMed: 27247223] Author\n[20]. Baker SG, Cook NR, Vickers A, Kramer BS, Using relative utility curves to evaluate risk\nprediction, J. R. Stat. Soc. A. Stat. Soc 172 (2009) 729\u2013748, 10.1111/j.1467-985X.2009.00592.x.\n[21]. Connell A, Black G, Montgomery H, Martin P, Nightingale C, King D, Karthikesalingam A, Manuscript\nHughes C, Back T, Ayoub K, Suleyman M, Jones G, Cross J, Stanley S, Emerson M, Merrick\nC, Rees G, Laing C, Raine R, Implementation of a Digitally Enabled Care Pathway (Part 2):\nQualitative Analysis of Experiences of Health Care Professionals, J. Med. Internet Res 21 (2019)\ne13143. [PubMed: 31368443]\n[22]. Wiens J, Saria S, Sendak M, Ghassemi M, Liu VX, Doshi-Velez F, Jung K, Heller K, Kale D,\nSaeed M, Ossorio PN, Thadaney-Israni S, Goldenberg A, Do no harm: a roadmap for responsible\nmachine learning for health care, Nat. Med 25 (2019) 1337\u20131340, 10.1038/s41591-019-0548-6.\n[PubMed: 31427808]\n[23]. Sendak MP, Balu S, Schulman KA, Barriers to Achieving Economies of Scale in Analysis\nof EHR Data, A Cautionary Tale, Applied Clinical Informatics. 8 (2017) 826\u2013831, 10.4338/\nACI-2017-03-CR-0046. [PubMed: 28837212]\nAuthor\n[24]. Hamrock E, Paige K, Parks J, Scheulen J, Levin S, Discrete event simulation for healthcare\norganizations: a tool for decision making, J. Healthc. Manag 58 (2013), 110\u2013124; discussion\n124\u2013125. [PubMed: 23650696]\nManuscript\n[25]. V\u00e1zquez-Serrano JI, Peimbert-Garc\u00eda RE, C\u00e1rdenas-Barr\u00f3n LE, Discrete-Event Simulation\nModeling in Healthcare: A Comprehensive Review, Int. J. Environ. Res. Public Health 18 (2021)\n12262, 10.3390/ijerph182212262. [PubMed: 34832016]\n[26]. Zhang X, Application of discrete event simulation in health care: a systematic review, BMC\nHealth Serv. Res 18 (2018) 687, 10.1186/s12913-018-3456-4. [PubMed: 30180848]\n[27]. Jacobson SH, Hall SN, Swisher JR, Discrete-Event Simulation of Health Care Systems, in: Hall\nRW (Ed.), Patient Flow: Reducing Delay in Healthcare Delivery, Springer US, Boston, MA,\n2006: pp. 211\u2013252. 10.1007/978-0-387-33636-7_8.\n[28]. Kovalchuk SV, Funkner AA, Metsker OG, Yakovlev AN, Simulation of patient flow in multiple\nhealthcare units using process and data mining techniques for model identification, J. Biomed.\nInform 82 (2018) 128\u2013142, 10.1016/j.jbi.2018.05.004. [PubMed: 29753874]\nAuthor\n[29]. Ko M, Chen E, Agrawal A, Rajpurkar P, Avati A, Ng A, Basu S, Shah NH, Improving hospital\nreadmission prediction using individualized utility analysis, J. Biomed. Inform 119 (2021),\n103826, 10.1016/j.jbi.2021.103826. [PubMed: 34087428]\n[30]. Bayati M, Braverman M, Gillam M, Mack KM, Ruiz G, Smith MS, Horvitz E, Data-Driven Manuscript\nDecisions for Reducing Readmissions for Heart Failure: General Methodology and Case Study,\nPLoS One 9 (2014) e109264.\n[31]. Mi\u0161\u00edc VV, Rajaram K, Gabel E, A simulation-based evaluation of machine learning models for\nclinical decision support: application and analysis using hospital readmission, npj Digital Med. 4\n(2021) 98, 10.1038/s41746-021-00468-7.\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 20\n[32]. Itoga NK, Minami HR, Chelvakumar M, Pearson K, Mell MM, Bendavid E, Owens DK,\nCost-effectiveness analysis of asymptomatic peripheral artery disease screening with the ABI\nAuthor\ntest, Vasc. Med 23 (2018) 97\u2013106, 10.1177/1358863X17745371. [PubMed: 29345540]\n[33]. Diao JA, Wedlund L, Kvedar J, Beyond performance metrics: modeling outcomes and cost for\nclinical machine learning, npj Digital Med. 4 (2021) 1\u20132, 10.1038/s41746-021-00495-4.\nManuscript\n[34]. Ghanzouri I, Amal S, Ho V, Safarnejad L, Cabot J, Brown-Johnson CG, Leeper N, Asch S,\nShah NH, Ross EG, Performance and usability testing of an automated tool for detection of\nperipheral artery disease using electronic health records, Sci. Rep 12 (2022) 13364, 10.1038/\ns41598-022-17180-5. [PubMed: 35922657]\n[35]. Cassandras CG, Lafortune S, Introduction to discrete event systems, Springer, New York, NY,\n2010, p. 2, ed., corr. at 2. print.\n[36]. Ghattas J, Peleg M, Soffer P, Denekamp Y, Learning the Context of a Clinical Process, in:\nRinderle-Ma S, Sadiq S, Leymann F (Eds.), Business Process Management Workshops, Springer,\nBerlin, Heidelberg, 2010, pp. 545\u2013556, 10.1007/978-3-642-12186-9_53.\n[37]. Choi J, Jansen K, Coenen A, Modeling a Nursing Guideline with Standard Terminology and\nUnified Modeling Language for a Nursing Decision Support System: A Case Study, AMIA Ann.\nSymp. Proc 2015 (2015) 426\u2013433. Author\n[38]. Ferrante S, Bonacina S, Pinciroli F, Modeling stroke rehabilitation processes using the\nUnified Modeling Language (UML), Comput. Biol. Med 43 (2013) 1390\u20131401, 10.1016/\nj.compbiomed.2013.07.012. [PubMed: 24034730] Manuscript\n[39]. Peleg M, 13 - Guidelines and workflow models, in: Greenes RA (Ed.), Clinical Decision Support,\nAcademic Press, Burlington, 2007, pp. 281\u2013306, 10.1016/B978-012369377-8/50014-3.\n[40]. Mulyar N, van der Aalst WMP, Peleg M, A pattern-based analysis of clinical computer-\ninterpretable guideline modeling languages, J. Am. Med. Inform. Assoc 14 (2007) 781\u2013787,\n10.1197/jamia.M2389. [PubMed: 17712087]\n[41]. Peleg M, Tu S, Manindroo A, Altman R, Modeling and analyzing biomedical processes\nusing Work-flow/Petri Net models and tools, Stud. Health Technol. Inform 107 (2004) 74\u201378,\n10.3233/978-1-60750-949-3-74. [PubMed: 15360778]\n[42]. Shahar Y, Young O, Shalom E, Galperin M, Mayaffit A, Moskovitch R, Hessing A, A framework\nfor a distributed, hybrid, multiple-ontology clinical-guideline library, and automated guideline-\nsupport tools, J. Biomed. Inform 37 (2004) 325\u2013344, 10.1016/j.jbi.2004.07.001. [PubMed: Author\n15488747]\n[43]. Ben-Kiki O, YAML Ain\u2019t Markup Language (YAMLTM) Version 1.1, (n.d.) 85.\n[44]. De Roock E, Martin N, Process mining in healthcare \u2013 An updated perspective on the state of the\nManuscript\nart, J. Biomed. Inform 127 (2022), 103995, 10.1016/j.jbi.2022.103995.\n[45]. Vickers AJ, Van Calster B, Steyerberg EW, Net benefit approaches to the evaluation of prediction\nmodels, molecular markers, and diagnostic tests, BMJ (2016), i6, 10.1136/bmj.i6. [PubMed:\n26810254]\n[46]. Baker SG, Putting Risk Prediction in Perspective: Relative Utility Curves, JNCI: Journal of the\nNational Cancer Institute. 101 (2009) 1538\u20131542, 10.1093/jnci/djp353. [PubMed: 19843888]\n[47]. McDermott MM, Greenland P, Liu K, Guralnik JM, Celic L, Criqui MH, Chan C, Martin GJ,\nSchneider J, Pearce WH, Taylor LM, Clark E, The Ankle Brachial Index Is Associated with Leg\nFunction and Physical Activity: The Walking and Leg Circulation Study, Ann. Intern. Med 136\n(2002) 873\u2013883, 10.7326/0003-4819-136-12-200206180-00008. [PubMed: 12069561]\n[48]. Hirsch AT, Criqui MH, Treat-Jacobson D, Regensteiner JG, Creager MA, Olin JW, Krook SH,\nAuthor\nHunninghake DB, Comerota AJ, Walsh ME, McDermott MM, Hiatt WR, Peripheral Arterial\nDisease Detection, Awareness, and Treatment in Primary Care, J. Am. Med. Assoc 286 (2001)\n1317\u20131324, 10.1001/jama.286.11.1317.\n[49]. Mahoney EM, Wang K, Cohen DJ, Hirsch AT, Alberts MJ, Eagle K, Mosse F, Jackson Manuscript\nJD, Steg PG, Bhatt DL, One-Year Costs in Patients With a History of or at Risk for\nAtherothrombosis in the United States, Circ. Cardiovasc. Qual. Outcomes 1 (2008) 38\u201345,\n10.1161/CIRCOUTCOMES.108.775247. [PubMed: 20031786]\n[50]. Aronow WS, Peripheral arterial disease of the lower extremities, Arch. Med. Sci 8 (2012) 375\u2013\n388, 10.5114/aoms.2012.28568. [PubMed: 22662015]\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 21\n[51]. Shu J, Santulli G, Update on peripheral artery disease: Epidemiology and evidence-based\nfacts, Atherosclerosis 275 (2018) 379\u2013381, 10.1016/j.atherosclerosis.2018.05.033. [PubMed:\nAuthor\n29843915]\n[52]. Chongthawonsatid S, Dutsadeevettakul S, Validity and reliability of the ankle-brachial index by\noscillometric blood pressure and automated ankle-brachial index, J. Res. Med. Sci 22 (2017) 44,\nManuscript\n10.4103/jrms.JRMS_728_16. [PubMed: 28567064]\n[53]. US Preventive Services Task Force, Curry SJ, Krist AH, Owens DK, Barry MJ, Caughey AB,\nDavidson KW, Doubeni CA, Epling JW, Kemper AR, Kubik M, Landefeld CS, Mangione CM,\nSilverstein M, Simon MA, Tseng C-W, Wong JB, Screening for Peripheral Artery Disease and\nCardiovascular Disease Risk Assessment With the Ankle-Brachial Index: US Preventive Services\nTask Force Recommendation Statement, JAMA 320 (2018) 177, 10.1001/jama.2018.8357.\n[PubMed: 29998344]\n[54]. Carspecken CW, Sharek PJ, Longhurst C, Pageler NM, A Clinical Case of Electronic Health\nRecord Drug Alert Fatigue: Consequences for Patient Outcome, Pediatrics 131 (2013) e1970\u2013\ne1973, 10.1542/peds.2012-3252. [PubMed: 23713099]\n[55]. Ancker JS, Edwards A, Nosal S, Hauser D, Mauer E, Kaushal R, Effects of workload, work\ncomplexity, and repeated alerts on alert fatigue in a clinical decision support system, BMC Med. Author\nInf. Decis. Making 17 (2017) 36, 10.1186/s12911-017-0430-8.\n[56]. van der Sijs H, Aarts J, Vulto A, Berg M, Overriding of Drug Safety Alerts in Computerized\nPhysician Order Entry, J. Am. Med. Inform. Assoc 13 (2006) 138\u2013147, 10.1197/jamia.M1809. Manuscript\n[PubMed: 16357358]\n[57]. Ankle Brachial Index, Stanford Medicine 25 (n.d.). https://stanfordmedicine25.stanford.edu/\nthe25/ankle-brachial-index.html (accessed September 5, 2022).\n[58]. Rojas E, Munoz-Gama J, Sep\u00falveda M, Capurro D, Process mining in healthcare: A literature\nreview, J. Biomed. Inform 61 (2016) 224\u2013236, 10.1016/j.jbi.2016.04.007. [PubMed: 27109932]\n[59]. Fujimoto R, Parallel and distributed simulation, in: Proceedings of the 2015 Winter Simulation\nConference, IEEE Press, Huntington Beach, California, 2015: pp. 45\u201359.\n[60]. Jafer S, Liu Q, Wainer G, Synchronization methods in parallel and distributed discrete-event\nsimulation, Simul. Model. Pract. Theory 30 (2013) 54\u201373, 10.1016/j.simpat.2012.08.003.\n[61]. Li RC, Asch SM, Shah NH, Developing a delivery science for artificial intelligence in healthcare,\nNPJ Digital Med. 3 (2020) 107, 10.1038/s41746-020-00318-y. Author\n[62]. Bedoya AD, Economou-Zavlanos NJ, Goldstein BA, Young A, Jelovsek JE, O\u2019Brien C, Parrish\nAB, Elengold S, Lytle K, Balu S, Huang E, Poon EG, Pencina MJ, A framework for the oversight\nand local deployment of safe and highquality prediction models, J. Am. Med. Inform. Assoc 29\nManuscript\n(2022) 1631\u20131636, 10.1093/jamia/ocac078. [PubMed: 35641123]\nAuthor\nManuscript\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 22\nStatement of Significance\nAuthor\nProblem The adoption of ML models into clinical workflows is lacking because traditional ML\nevaluation metrics fail to accurately assess how useful a model will be in practice.\nManuscript\nWhat is Prior work has simulated individual model impact in the context of specific care\nAlready delivery workflows. However, these efforts have limited generalizability to other models/\nKnown workflows and exhibit overreliance on non-modifiable assumptions.\nWhat This Our contribution builds on prior work through the development of a flexible, reusable set\nPaper Adds of methods that allow for the systematic quantification of the usefulness of ML models by\nsimulating their corresponding care management workflows. The APLUS library can help\nhospitals to better evaluate which models are worthy of deployment and identify the best\nstrategies for integrating such models into clinical workflows.\nAuthor\nManuscript\nAuthor\nManuscript\nAuthor\nManuscript\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 23\nAuthor\nManuscript\nAuthor\nFig. 1.\nStates, transitions, and transition conditions for the (a) nurse-driven workflow and (b)\ndoctor-driven workflow. All patients begin at the \u201cPatient Visits Clinic\u201d state in the top\nManuscript\nleft of the charts. Then, patients progress according to their individual-level properties, and\nend at one of 3 treatment options: \u201cUntreated\u201d, \u201cMedication\u201d, or \u201cSurgery\u201d. Trapezoids\nrepresent capacity constraints, diamonds represent decision points, squares are intermediate\nstates, and pills are end states.\nAuthor\nManuscript\nAuthor\nManuscript\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 24\nAuthor\nManuscript\nFig. 2. Author\n(a) Utility achieved by the nurse-driven workflow under thresholded screening across\ndifferent nurse capacities using the optimal model cutoff threshold. (b) Utility achieved\nby the nurse-driven workflow under ranked screening across different nurse capacities. The Manuscript\ndeep learning model is most differentiated under a thresholded screening strategy, and only\nat high nurse capacity levels. All plots assume unlimited specialist capacity.\nAuthor\nManuscript\nAuthor\nManuscript\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 25\nAuthor\nManuscript\nFig. 3.\nUtility achieved by the (a) deep learning model, (b) random forest model, and (c) logistic\nregression across various model cutoff thresholds. The sharper peaks in the random forest\nand logistic regression plots indicate that the probability distributions they learn have\nAuthor\nmore dispersion than that learned by the deep learning model. All plots assume unlimited\nspecialist capacity and a thresholded screening strategy.\nManuscript\nAuthor\nManuscript\nAuthor\nManuscript\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 26\nAuthor\nManuscript\nAuthor\nManuscript\nFig. 4.\nThis plot shows the utility achieved by the doctor-driven workflow across different levels of\nAuthor\nalert fatigue using a model cutoff threshold of 0.5, assuming unlimited specialist capacity.\nManuscript\nAuthor\nManuscript\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 27\nAuthor\nManuscript\nFig. 5. Author\n(a) Utility achieved by the nurse-driven workflow under thresholded screening across\ndifferent nurse capacities using the optimal model cutoff threshold. (b) Utility achieved by\nthe nurse-driven workflow under ranked screening across different nurse capacities. We see Manuscript\nthat the achievable utility of all models is unaffected by increases in nurse capacity beyond\n3\u20134 nurses. All plots assume a specialist capacity of 2 patients/day.\nAuthor\nManuscript\nAuthor\nManuscript\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 28\nAuthor\nManuscript\nAuthor\nManuscript\nFig. 6.\nThis plot shows the utility achieved by the doctor-driven workflow across different levels\nAuthor\nof alert fatigue using a model cutoff threshold of 0.5, assuming a specialist capacity of 2\npatients/day.\nManuscript\nAuthor\nManuscript\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29.Wornow et al. Page 29\nAuthor\nManuscript\nAuthor\nManuscript\nAuthor\nManuscript\nFig. 7.\nThis heatmap shows the incremental gain from using the nurse-driven workflow over a\ndoctor-driven workflow at a given capacity level for each workflow, assuming a specialist\ncapacity of 5 patients/day. The y-axis represents capacity for the nurse-driven workflow, and\nthe x-axis represents the probability that a doctor reads an EHR alert in the doctor-driven\nworkflow. The value of the cell at coordinates (i, j) in the heatmap shows the incremental\ngain in achievable utility that can be expected by using a nurse-driven workflow with\ncapacity i instead of a doctor-driven workflow with an alert fatigue level of j. Thus, positive Author\nnumbers (i.e. red cells) indicate that the nurse-driven workflow is preferable to the doctor-\ndriven workflow at their corresponding capacity levels, while negative numbers (i.e. blue\nManuscript\ncells) are situations in which the doctor-driven workflow should be preferred. That is why\nthe top rows, which show nurse capacity at its highest, are dark red, while the farright rows,\nwhich represent the highest probability that doctors read their EHR alerts, are dark blue.\nJ Biomed Inform. Author manuscript; available in PMC 2023 June 29."
}